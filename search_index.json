[
["index.html", "Introduction to Biomedical Data Science Chapter 1 Preface", " Introduction to Biomedical Data Science Stephen Turner &amp; VP Nagraj 2017-02-11 Chapter 1 Preface In this textbook, the authors aim to introduce the R statistical computing environment and packages for manipulating and visualizing high-dimensional data, covers strategies for reproducible research, and culminates with analysis of data from a real RNA-seq experiment using R and Bioconductor packages. "],
["r-basics.html", "Chapter 2 R Basics 2.1 RStudio 2.2 Basic operations 2.3 Functions 2.4 Data Frames", " Chapter 2 R Basics This section introduces the R environment and some of the most basic funcionality aspects of R that are used through the remainder of the class. This section assumes little to no experience with statistical computing with R. We will introduce the R statistical computing environment, RStudio, and the dataset that we will work with for the remainder of the lesson. We will cover very basic functionality in R, including variables, functions, and importing/inspecting data frames. Make sure you complete the setup here prior to the class. 2.1 RStudio Let’s start by learning about RStudio. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment that makes using R much easier. Panes in RStudio. There are four panes, and their orientation is configurable under “Tools – Global Options.” You don’t have to do it this way, but I usually set up my window to have: Editor in the top left Console top right Environment/history on the bottom left Plots/help on the bottom right. Projects: first, start a new project in a new folder somewhere easy to remember. When we start reading in data it’ll be important that the code and the data are in the same place. Creating a project creates an Rproj file that opens R running in that folder. This way, when you want to read in dataset whatever.txt, you just tell it the filename rather than a full path. This is critical for reproducibility, and we’ll talk about that more later. Code that you type into the console is code that R executes. From here forward we will use the editor window to write a script that we can save to a file and run it again whenever we want to. We usually give it a .R extension, but it’s just a plain text file. If you want to send commands from your editor to the console, use CMD+Enter (Ctrl+Enter on Windows). Anything after a # sign is a comment. Use them liberally to comment your code. 2.2 Basic operations R can be used as a glorified calculator. Try typing this in directly into the console. Make sure you’re typing into into the editor, not the console, and save your script. Use the run button, or press CMD+Enter (Ctrl+Enter on Windows). 2+2 5*4 2^3 R Knows order of operations and scientific notation. 2+3*4/(5+3)*15/2^2+3*4^2 5e4 However, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- and the value we want to give it: weight_kg &lt;- 55 &lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Mostly similar to = but not always. Learn to use &lt;- as it is good programming practice. Using = in place of &lt;- can lead to issues down the line. The keyboard shortcut for inserting the &lt;- operator is Alt-dash. Objects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). In doubt check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names. When assigning a value to an object, R does not print anything. You can force to print the value by typing the name: weight_kg Now that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg). 2.2 * weight_kg We can also change a variable’s value by assigning it a new one: weight_kg &lt;- 57.5 2.2 * weight_kg This means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable. weight_lb &lt;- 2.2 * weight_kg and then change weight_kg to 100. weight_kg &lt;- 100 What do you think is the current content of the object weight_lb? 126.5 or 220? You can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment. ls() rm(weight_lb, weight_kg) ls() weight_lb # oops! you should get an error because weight_lb no longer exists! EXERCISE 1 What are the values after each statement in the following? mass &lt;- 50 # mass? age &lt;- 30 # age? mass &lt;- mass * 2 # mass? age &lt;- age - 10 # age? mass_index &lt;- mass/age # massIndex? 2.3 Functions R has built-in functions. # Notice that this is a comment. # Anything behind a # is &quot;commented out&quot; and is not run. sqrt(144) log(1000) Get help by typing a question mark in front of the function’s name, or help(functionname): help(log) ?log Note syntax highlighting when typing this into the editor. Also note how we pass arguments to functions. The base= part inside the parentheses is called an argument, and most functions use arguments. Arguments modify the behavior of the function. Functions some input (e.g., some data, an object) and other options to change what the function will return, or how to treat the data provided. Finally, see how you can next one function inside of another (here taking the square root of the log-base-10 of 1000). log(1000) log(1000, base=10) log(1000, 10) sqrt(log(1000, base=10)) EXERCISE 2 See ?abs and calculate the square root of the log-base-10 of the absolute value of -4*(2550-50). Answer should be 2. 2.4 Data Frames There are lots of different basic data structures in R. If you take any kind of longer introduction to R you’ll probably learn about arrays, lists, matrices, etc. We are going to skip straight to the data structure you’ll probably use most – the data frame. We use data frames to store heterogeneous tabular data in R: tabular, meaning that individuals or observations are typically represented in rows, while variables or features are represented as columns; heterogeneous, meaning that columns/features/variables can be different classes (on variable, e.g. age, can be numeric, while another, e.g., cause of death, can be text). Let’s move on to learning about data frames. "],
["data-frames-1.html", "Chapter 3 Data Frames 3.1 Our data 3.2 Reading in data 3.3 Inspecting data.frame objects 3.4 Accessing variables &amp; subsetting data frames 3.5 BONUS: Preview to advanced manipulation", " Chapter 3 Data Frames There are lots of different basic data structures in R. If you take any kind of longer introduction to R you’ll probably learn about arrays, lists, matrices, etc. Let’s skip straight to the data structure you’ll probably use most – the data frame. We use data frames to store heterogeneous tabular data in R: tabular, meaning that individuals or observations are typically represented in rows, while variables or features are represented as columns; heterogeneous, meaning that columns/features/variables can be different classes (on variable, e.g. age, can be numeric, while another, e.g., cause of death, can be text). This lesson assumes a basic familiarity with R. 3.1 Our data The data we’re going to look at is cleaned up version of a gene expression dataset from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. This data is from a gene expression microarray, and in this paper the authors are examining the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that: Raise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions. Respond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient. You can download the cleaned up version of the data at the link above. The file is called brauer2007_tidy.csv. Later on we’ll actually start with the original raw data (minimally processed) and manipulate it so that we can make it more amenable for analysis. 3.2 Reading in data 3.2.1 dplyr and readr There are some built-in functions for reading in data in text files. These functions are read-dot-something – for example, read.csv() reads in comma-delimited text data; read.delim() reads in tab-delimited text, etc. We’re going to read in data a little bit differently here using the readr package. When you load the readr package, you’ll have access to very similar looking functions, named read-underscore-something – e.g., read_csv(). You have to have the readr package installed to access these functions. Compared to the base functions, they’re much faster, they’re good at guessing the types of data in the columns, they don’t do some of the other silly things that the base functions do. We’re going to use another package later on called dplyr, and if you have the dplyr package loaded as well, and you read in the data with readr, the data will display nicely. First let’s load those packages. library(readr) library(dplyr) If you see a warning that looks like this: Error in library(packageName) : there is no package called 'packageName', then you don’t have the package installed correctly. See the setup page. 3.2.2 read_csv() Now, let’s actually load the data. You can get help for the import function with ?read_csv. When we load data we assign it to a variable just like any other, and we can choose a name for that data. Since we’re going to be referring to this data a lot, let’s give it a short easy name to type. I’m going to call it ydat. Once we’ve loaded it we can type the name of the object itself (ydat) to see it printed to the screen. ydat &lt;- read_csv(file=&quot;http://bioconnector.org/data/brauer2007_tidy.csv&quot;) ydat Take a look at that output. The nice thing about loading dplyr and reading in data with readr is that data frames are displayed in a much more friendly way. This dataset has nearly 200,000 rows and 7 columns. When you import data this way and try to display the object in the console, instead of trying to display all 200,000 rows, you’ll only see about 10 by default. Also, if you have so many columns that the data would wrap off the edge of your screen, those columns will not be displayed, but you’ll see at the bottom of the output which, if any, columns were hidden from view. If you want to see the whole dataset, there are two ways to do this. First, you can click on the name of the data.frame in the Environment panel in RStudio. Or you could use the View() function (with a capital V). View(ydat) 3.3 Inspecting data.frame objects There are several built-in functions that are useful for working with data frames. Content: head(): shows the first few rows tail(): shows the last few rows Size: dim(): returns a 2-element vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object) nrow(): returns the number of rows ncol(): returns the number of columns Summary: colnames() (or just names()): returns the column names str(): structure of the object and information about the class, length and content of each column summary(): works differently depending on what kind of object you pass to it. Passing a data frame to the summary() function prints out useful summary statistics about numeric column (min, max, median, mean, etc.) head(ydat) tail(ydat) dim(ydat) names(ydat) str(ydat) summary(ydat) 3.4 Accessing variables &amp; subsetting data frames We can access individual variables within a data frame using the $ operator, e.g., mydataframe$specificVariable. Let’s print out all the gene names in the data. Then let’s calculate the average expression across all conditions, all genes (using the built-in mean() function). # display all gene symbols ydat$symbol #mean expression mean(ydat$expression) Now that’s not too interesting. This is the average gene expression across all genes, across all conditions. The data is actually scaled/centered around zero: We might be interested in the average expression of genes with a particular biological function, and how that changes over different growth rates restricted by particular nutrients. This is the kind of thing we’re going to do in the next section. EXERCISE 1 What’s the standard deviation expression (hint: get help on the sd function with ?sd). What’s the range of rate represented in the data? (hint: range()). 3.5 BONUS: Preview to advanced manipulation What if we wanted show the mean expression, standard deviation, and correlation between growth rate and expression, separately for each limiting nutrient, separately for each gene, for all genes involved in the leucine biosynthesis pathway? ydat %&gt;% filter(bp==&quot;leucine biosynthesis&quot;) %&gt;% group_by(nutrient, symbol) %&gt;% summarize(mean=mean(expression), sd=sd(expression), r=cor(rate, expression)) Neat eh? Let’s learn how to do that in the advanced manipulation with dplyr lesson. "],
["data-manipulation-with-dplyr.html", "Chapter 4 Data Manipulation with dplyr 4.1 Review 4.2 The dplyr package 4.3 dplyr verbs 4.4 The pipe: %&gt;% 4.5 Homework", " Chapter 4 Data Manipulation with dplyr Data analysis involves a large amount of janitor work – munging and cleaning data to facilitate downstream data analysis. This lesson demonstrates techniques for advanced data manipulation and analysis with the split-apply-combine strategy. We will use the dplyr package in R to effectively manipulate and conditionally compute summary statistics over subsets of a “big” dataset containing many observations. This lesson assumes a basic familiarity with R and data frames. 4.1 Review 4.1.1 Our data We’re going to use the yeast gene expression dataset described on the data frames lesson. This is a cleaned up version of a gene expression dataset from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. This data is from a gene expression microarray, and in this paper the authors are examining the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that: Raise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions. Respond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient. You can download the cleaned up version of the data at the link above. The file is called brauer2007_tidy.csv. Later on we’ll actually start with the original raw data (minimally processed) and manipulate it so that we can make it more amenable for analysis. 4.1.2 Reading in data We need to load both the dplyr and readr packages for efficiently reading in and displaying this data. We’re also going to use many other functions from the dplyr package. Make sure you have these packages installed as described on the setup page. # Load packages library(readr) library(dplyr) # Read in data ydat &lt;- read_csv(file=&quot;data/brauer2007_tidy.csv&quot;) # Display the data ydat # Optionally, bring up the data in a viewer window # View(ydat) ## # A tibble: 198,430 × 7 ## symbol systematic_name nutrient rate expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SFB2 YNL049C Glucose 0.05 -0.24 ## 2 &lt;NA&gt; YNL095C Glucose 0.05 0.28 ## 3 QRI7 YDL104C Glucose 0.05 -0.02 ## 4 CFT2 YLR115W Glucose 0.05 -0.33 ## 5 SSO2 YMR183C Glucose 0.05 0.05 ## 6 PSP2 YML017W Glucose 0.05 -0.69 ## 7 RIB2 YOL066C Glucose 0.05 -0.55 ## 8 VMA13 YPR036W Glucose 0.05 -0.75 ## 9 EDC3 YEL015W Glucose 0.05 -0.24 ## 10 VPS5 YOR069W Glucose 0.05 -0.16 ## # ... with 198,420 more rows, and 2 more variables: bp &lt;chr&gt;, mf &lt;chr&gt; 4.2 The dplyr package The dplyr package is a relatively new R package that makes data manipulation fast and easy. It imports functionality from another package called magrittr that allows you to chain commands together into a pipeline that will completely change the way you write R code such that you’re writing code the way you’re thinking about the problem. When you read in data with the readr package (read_csv()) and you had the dplyr package loaded already, the data frame takes on this “special” class of data frames called a tbl, which you can see with class(ydat). If you have other “regular” data frames in your workspace, the tbl_df() function will convert it into the special dplyr tbl that displays nicely (e.g.: iris &lt;- tbl_df(iris)). You don’t have to turn all your data frame objects into tbl_df objects, but it does make working with large datasets a bit easier. 4.3 dplyr verbs The dplyr package gives you a handful of useful verbs for managing data. On their own they don’t do anything that base R can’t do. Here are some of the single-table verbs we’ll be working with in this lesson (single-table meaning that they only work on a single table – contrast that to two-table verbs used for joining data together, which we’ll cover in a later lesson). filter() select() mutate() arrange() summarize() group_by() They all take a data.frame or tbl_df as their input for the first argument, and they all return a data.frame or tbl_df as output. 4.3.1 filter() If you want to filter rows of the data where some condition is true, use the filter() function. The first argument is the data frame you want to filter, e.g. filter(mydata, .... The second argument is a condition you must satisfy, e.g. filter(ydat, symbol == &quot;LEU1&quot;). If you want to satisfy all of multiple conditions, you can use the “and” operator, &amp;. The “or” operator | (the pipe character, usually shift-backslash) will return a subset that meet any of the conditions. ==: Equal to !=: Not equal to &gt;, &gt;=: Greater than, greater than or equal to &lt;, &lt;=: Less than, less than or equal to Let’s try it out. For this to work you have to have already loaded the dplyr package. Let’s take a look at LEU1, a gene involved in leucine synthesis. # First, make sure you&#39;ve loaded the dplyr package library(dplyr) # Look at a single gene involved in leucine synthesis pathway filter(ydat, symbol == &quot;LEU1&quot;) # Optionally, bring that result up in a View window # View(filter(ydat, symbol == &quot;LEU1&quot;)) # Look at multiple genes filter(ydat, symbol==&quot;LEU1&quot; | symbol==&quot;ADH2&quot;) # Look at LEU1 expression at a low growth rate due to nutrient depletion # Notice how LEU1 is highly upregulated when leucine is depleted! filter(ydat, symbol==&quot;LEU1&quot; &amp; rate==.05) # But expression goes back down when the growth/nutrient restriction is relaxed filter(ydat, symbol==&quot;LEU1&quot; &amp; rate==.3) # Show only stats for LEU1 and Leucine depletion. # LEU1 expression starts off high and drops filter(ydat, symbol==&quot;LEU1&quot; &amp; nutrient==&quot;Leucine&quot;) # What about LEU1 expression with other nutrients being depleted? filter(ydat, symbol==&quot;LEU1&quot; &amp; nutrient==&quot;Glucose&quot;) Let’s look at this graphically. Don’t worry about what these commands are doing just yet - we’ll cover that later on when we talk about ggplot2. Here’s I’m taking the filtered dataset containing just expression estimates for LEU1 where I have 36 rows (one for each of 6 nutrients \\(\\times\\) 6 growth rates), and I’m piping that dataset to the plotting function, where I’m plotting rate on the x-axis, expression on the y-axis, mapping the value of nutrient to the color, and using a line plot to display the data. library(ggplot2) filter(ydat, symbol==&quot;LEU1&quot;) %&gt;% ggplot(aes(rate, expression, colour=nutrient)) + geom_line(lwd=1.5) Look closely at that! LEU1 is highly expressed when starved of leucine because the cell has to synthesize its own! And as the amount of leucine in the environment (the growth rate) increases, the cell can worry less about synthesizing leucine, so LEU1 expression goes back down. Consequently the cell can devote more energy into other functions, and we see other genes’ expression very slightly raising. EXERCISE 1 Display the data where the gene ontology biological process (the bp variable) is “leucine biosynthesis” (case-sensitive) and the limiting nutrient was Leucine. (Answer should return a 24-by-7 data frame – 4 genes \\(\\times\\) 6 growth rates). Gene/rate combinations had high expression (in the top 1% of expressed genes)? Hint: see ?quantile and try quantile(ydat$expression, probs=.99) to see the expression value which is higher than 99% of all the data, then filter() based on that. Try wrapping your answer with a View() function so you can see the whole thing. What does it look like those genes are doing? Answer should return a 1971-by-7 data frame. 4.3.1.1 Aside: Writing Data to File What we’ve done up to this point is read in data from a file (read_csv(...)), and assigning that to an object in our workspace (ydat &lt;- ...). When we run operations like filter() on our data, consider two things: The ydat object in our workspace is not being modified directly. That is, we can filter(ydat, ...), and a result is returned to the screen, but ydat remains the same. This effect is similar to what we demonstrated in our first session. # Assign the value &#39;50&#39; to the weight object. weight &lt;- 50 # Print out weight to the screen (50) weight # What&#39;s the value of weight plus 10? weight + 10 # Weight is still 50 weight # Weight is only modified if we *reassign* weight to the modified value weight &lt;- weight+10 # Weight is now 60 weight More importantly, the data file on disk (data/brauer2007_tidy.csv) is never modified. No matter what we do to ydat, the file is never modified. If we want to save the result of an operation to a file on disk, we can assign the result of an operation to an object, and write_csv that object to disk. See the help for ?write_csv (note, write_csv() with an underscore is part of the readr package – not to be confused with the built-in write.csv() function). # What&#39;s the result of this filter operation? filter(ydat, nutrient==&quot;Leucine&quot; &amp; bp==&quot;leucine biosynthesis&quot;) # Assign the result to a new object leudat &lt;- filter(ydat, nutrient==&quot;Leucine&quot; &amp; bp==&quot;leucine biosynthesis&quot;) # Write that out to disk write_csv(leudat, &quot;leucinedata.csv&quot;) Note that this is different than saving your entire workspace to an Rdata file, which would contain all the objects we’ve created (weight, ydat, leudat, etc). 4.3.2 select() The filter() function allows you to return only certain rows matching a condition. The select() function returns only certain columns. The first argument is the data, and subsequent arguments are the columns you want. # Select just the symbol and systematic_name select(ydat, symbol, systematic_name) # Alternatively, just remove columns. Remove the bp and mf columns. select(ydat, -bp, -mf) # Notice that the original data doesn&#39;t change! ydat Notice above how the original data doesn’t change. We’re selecting out only certain columns of interest and throwing away columns we don’t care about. If we wanted to keep this data, we would need to reassign the result of the select() operation to a new object. Let’s make a new object called nogo that does not contain the GO annotations. Notice again how the original data is unchanged. # create a new dataset without the go annotations. nogo &lt;- select(ydat, -bp, -mf) nogo # we could filter this new dataset filter(nogo, symbol==&quot;LEU1&quot; &amp; rate==.05) # Notice how the original data is unchanged - still have all 7 columns ydat 4.3.3 mutate() The mutate() function adds new columns to the data. Remember, it doesn’t actually modify the data frame you’re operating on, and the result is transient unless you assign it to a new object or reassign it back to itself (generally, not always a good practice). The expression level reported here is the \\(log_2\\) of the sample signal divided by the signal in the reference channel, where the reference RNA for all samples was taken from the glucose-limited chemostat grown at a dilution rate of 0.25 \\(h^{-1}\\). Let’s mutate this data to add a new variable called “signal” that’s the actual raw signal ratio instead of the log-transformed signal. mutate(nogo, signal=2^expression) Mutate has a nice little feature too in that it’s “lazy.” You can mutate and add one variable, then continue mutating to add more variables based on that variable. Let’s make another column that’s the square root of the signal ratio. mutate(nogo, signal=2^expression, sigsr=sqrt(signal)) Again, don’t worry about the code here to make the plot – we’ll learn about this later. Why do you think we log-transform the data prior to analysis? library(tidyr) mutate(nogo, signal=2^expression, sigsr=sqrt(signal)) %&gt;% gather(unit, value, expression:sigsr) %&gt;% ggplot(aes(value)) + geom_histogram(bins=100) + facet_wrap(~unit, scales=&quot;free&quot;) 4.3.4 arrange() The arrange() function does what it sounds like. It takes a data frame or tbl and arranges (or sorts) by column(s) of interest. The first argument is the data, and subsequent arguments are columns to sort on. Use the desc() function to arrange by descending. # arrange by gene symbol arrange(ydat, symbol) # arrange by expression (default: increasing) arrange(ydat, expression) # arrange by decreasing expression arrange(ydat, desc(expression)) EXERCISE 2 First, re-run the command you used above to filter the data for genes involved in the “leucine biosynthesis” biological process and where the limiting nutrient is Leucine. Wrap this entire filtered result with a call to arrange() where you’ll arrange the result of #1 by the gene symbol. Wrap this entire result in a View() statement so you can see the entire result. 4.3.5 summarize() The summarize() function summarizes multiple values to a single value. On its own the summarize() function doesn’t seem to be all that useful. The dplyr package provides a few convenience functions called n() and n_distinct() that tell you the number of observations or the number of distinct values of a particular variable. Notice that summarize takes a data frame and returns a data frame. In this case it’s a 1x1 data frame with a single row and a single column. The name of the column, by default is whatever the expression was used to summarize the data. This usually isn’t pretty, and if we wanted to work with this resulting data frame later on, we’d want to name that returned value something easier to deal with. # Get the mean expression for all genes summarize(ydat, mean(expression)) # Use a more friendly name, e.g., meanexp, or whatever you want to call it. summarize(ydat, meanexp=mean(expression)) # Measure the correlation between rate and expression summarize(ydat, r=cor(rate, expression)) # Get the number of observations summarize(ydat, n()) # The number of distinct gene symbols in the data summarize(ydat, n_distinct(symbol)) 4.3.6 group_by() We saw that summarize() isn’t that useful on its own. Neither is group_by() All this does is takes an existing data frame and coverts it into a grouped data frame where operations are performed by group. ydat group_by(ydat, nutrient) group_by(ydat, nutrient, rate) The real power comes in where group_by() and summarize() are used together. First, write the group_by() statement. Then wrap the result of that with a call to summarize(). # Get the mean expression for each gene # group_by(ydat, symbol) summarize(group_by(ydat, symbol), meanexp=mean(expression)) # Get the correlation between rate and expression for each nutrient # group_by(ydat, nutrient) summarize(group_by(ydat, nutrient), r=cor(rate, expression)) 4.4 The pipe: %&gt;% 4.4.1 How %&gt;% works This is where things get awesome. The dplyr package imports functionality from the magrittr package that lets you pipe the output of one function to the input of another, so you can avoid nesting functions. It looks like this: %&gt;%. You don’t have to load the magrittr package to use it since dplyr imports its functionality when you load the dplyr package. Here’s the simplest way to use it. Remember the tail() function. It expects a data frame as input, and the next argument is the number of lines to print. These two commands are identical: tail(ydat, 5) ydat %&gt;% tail(5) Let’s use one of the dplyr verbs. filter(ydat, nutrient==&quot;Leucine&quot;) ydat %&gt;% filter(nutrient==&quot;Leucine&quot;) 4.4.2 Nesting versus %&gt;% So what? Now, think about this for a minute. What if we wanted to get the correlation between the growth rate and expression separately for each limiting nutrient only for genes in the leucine biosynthesis pathway, and return a sorted list of those correlation coeffients rounded to two digits? Mentally we would do something like this: Take the ydat dataset then filter() it for genes in the leucine biosynthesis pathway then group_by() the limiting nutrient then summarize() to get the correlation (cor()) between rate and expression then mutate() to round the result of the above calculation to two significant digits then arrange() by the rounded correlation coefficient above But in code, it gets ugly. First, take the ydat dataset ydat then filter() it for genes in the leucine biosynthesis pathway filter(ydat, bp==&quot;leucine biosynthesis&quot;) then group_by() the limiting nutrient group_by(filter(ydat, bp==&quot;leucine biosynthesis&quot;), nutrient) then summarize() to get the correlation (cor()) between rate and expression summarize(group_by(filter(ydat, bp == &quot;leucine biosynthesis&quot;), nutrient), r = cor(rate, expression)) then mutate() to round the result of the above calculation to two significant digits mutate(summarize(group_by(filter(ydat, bp == &quot;leucine biosynthesis&quot;), nutrient), r = cor(rate, expression)), r = round(r, 2)) then arrange() by the rounded correlation coefficient above arrange( mutate( summarize( group_by( filter(ydat, bp==&quot;leucine biosynthesis&quot;), nutrient), r=cor(rate, expression)), r=round(r, 2)), r) Now compare that with the mental process of what you’re actually trying to accomplish. The way you would do this without pipes is completely inside-out and backwards from the way you express in words and in thought what you want to do. The pipe operator %&gt;% allows you to pass the output data frame from one function to the input data frame to another function. Nesting functions versus piping This is how we would do that in code. It’s as simple as replacing the word “then” in words to the symbol %&gt;% in code. (There’s a keyboard shortcut that I’ll use frequently to insert the %&gt;% sequence – you can see what it is by clicking the Tools menu in RStudio, then selecting Keyboard Shortcut Help. On Mac, it’s CMD-SHIFT-M.) ydat %&gt;% filter(bp==&quot;leucine biosynthesis&quot;) %&gt;% group_by(nutrient) %&gt;% summarize(r=cor(rate, expression)) %&gt;% mutate(r=round(r,2)) %&gt;% arrange(r) 4.4.3 Piping exercises EXERCISE 3 Here’s a warm-up round. Try the following. Show the limiting nutrient and expression values for the gene ADH2 when the growth rate is restricted to 0.05. Hint: 2 pipes: filter and select. What are the four most highly expressed genes when the growth rate is restricted to 0.05 by restricting glucose? Show only the symbol, expression value, and GO terms. Hint: 4 pipes: filter, arrange, head, and select. When the growth rate is restricted to 0.05, what is the average expression level across all genes in the “response to stress” biological process, separately for each limiting nutrient? What about genes in the “protein biosynthesis” biological process? Hint: 3 pipes: filter, group_by, summarize. EXERCISE 4 That was easy, right? How about some tougher ones. First, some review. How do we see the number of distinct values of a variable? Use n_distinct() within a summarize() call. ydat %&gt;% summarize(n_distinct(mf)) Which 10 biological process annotations have the most genes associated with them? What about molecular functions? Hint: 4 pipes: group_by, summarize with n_distinct, arrange, head. How many distinct genes are there where we know what process the gene is involved in but we don’t know what it does? Hint: 3 pipes; filter where bp!=&quot;biological process unknown&quot; &amp; mf==&quot;molecular function unknown&quot;, and after selecting columns of interest, pipe the output to distinct(). The answer should be 737, and here are a few: When the growth rate is restricted to 0.05 by limiting Glucose, which biological processes are the most upregulated? Show a sorted list with the most upregulated BPs on top, displaying the biological process and the average expression of all genes in that process rounded to two digits. Hint: 5 pipes: filter, group_by, summarize, mutate, arrange. Group the data by limiting nutrient (primarily) then by biological process. Get the average expression for all genes annotated with each process, separately for each limiting nutrient, where the growth rate is restricted to 0.05. Arrange the result to show the most upregulated processes on top. The initial result will look like the result below. Pipe this output to a View() statement. What’s going on? Why didn’t the arrange() work? Hint: 5 pipes: filter, group_by, summarize, arrange, View. Let’s try to further process that result to get only the top three most upregulated biolgocal processes for each limiting nutrient. Google search “dplyr first result within group.” You’ll need a filter(row_number()......) in there somewhere. Hint: 5 pipes: filter, group_by, summarize, arrange, filter(row_number().... Note: dplyr’s pipe syntax used to be %.% before it changed to %&gt;%. So when looking around, you might still see some people use the old syntax. Now if you try to use the old syntax, you’ll get a deprecation warning. There’s a slight problem with the examples above. We’re getting the average expression of all the biological processes separately by each nutrient. But some of these biological processes only have a single gene in them! If we tried to do the same thing to get the correlation between rate and expression, the calculation would work, but we’d get a warning about a standard deviation being zero. The correlation coefficient value that results is NA, i.e., missing. While we’re summarizing the correlation between rate and expression, let’s also show the number of distinct genes within each grouping. ydat %&gt;% group_by(nutrient, bp) %&gt;% summarize(r=cor(rate, expression), ngenes=n_distinct(symbol)) ## Warning in cor(c(0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, ## 0.05, : the standard deviation is zero Take the above code and continue to process the result to show only results where the process has at least 5 genes. Add a column corresponding to the absolute value of the correlation coefficient, and show for each nutrient the singular process with the highest correlation between rate and expression, regardless of direction. Hint: 4 more pipes: filter, mutate, arrange, and filter again with row_number()==1. Ignore the warning. 4.5 Homework Looking for more practice? Try this homework assignment. "],
["tidy-data-and-tidyr.html", "Chapter 5 Tidy Data and tidyr 5.1 Review 5.2 Tidy data 5.3 The tidyr package 5.4 Tidy the yeast data", " Chapter 5 Tidy Data and tidyr Recommended reading prior to class: Sections 1-3 of Wickham, H. “Tidy Data.” Journal of Statistical Software 59:10 (2014). 5.1 Review 5.1.1 Prior classes R basics Data frames Manipulating data with dplyr and %&gt;% 5.1.2 Data needed Hit the “Data” link above or use the direct links below to download the following datasets, saving them in a data folder relative to your current working RStudio project: Heart rate data: heartrate2dose.csv Tidy yeast data: brauer2007_tidy.csv Original (untidy) yeast data: brauer2007_messy.csv Yeast systematic names to GO terms: brauer2007_sysname2go.csv 5.2 Tidy data So far we’ve dealt exclusively with tidy data – data that’s easy to work with, manipulate, and visualize. That’s because our dataset has two key properties: Each column is a variable. Each row is an observation. You can read a lot more about tidy data in this paper. Let’s load some untidy data and see if we can see the difference. This is some made-up data for five different patients (Jon, Ann, Bill, Kate, and Joe) given three different drugs (A, B, and C), at two doses (10 and 20), and measuring their heart rate. Download the heartrate2dose.csv file directly from the data downloads page. Load readr and dplyr, and import and display the data. library(readr) library(dplyr) hr &lt;- read_csv(&quot;data/heartrate2dose.csv&quot;) hr Notice how with the yeast data each variable (symbol, nutrient, rate, expression, etc.) were each in their own column. In this heart rate data, we have four variables: name, drug, dose, and heart rate. Name is in a column, but drug is in the header row. Furthermore the drug and dose are tied together in the same column, and the heart rate is scattered around the entire table. If we wanted to do things like filter the dataset where drug==&quot;a&quot; or dose==20 or heartrate&gt;=80 we couldn’t do it because these variables aren’t in columns. 5.3 The tidyr package The tidyr package helps with this. There are several functions in the tidyr package but the ones we’re going to use are separate() and gather(). The gather() function takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer. The separate() function separates one column into multiple columns. So, what we need to do is gather all the drug/dose data into a column with their corresponding heart rate, and then separate that column into two separate columns for the drug and dose. Before we get started, load the tidyr package, and look at the help pages for ?gather and ?separate. Notice how each of these functions takes a data frame as input and returns a data frame as output. Thus, we can pipe from one function to the next. library(tidyr) 5.3.1 gather() The help for ?gather tells us that we first pass in a data frame (or omit the first argument, and pipe in the data with %&gt;%). The next two arguments are the names of the key and value columns to create, and all the relevant arguments that come after that are the columns we want to gather together. Here’s one way to do it. hr %&gt;% gather(key=drugdose, value=hr, a_10, a_20, b_10, b_20, c_10, c_20) But that gets cumbersome to type all those names. What if we had 100 drugs and 3 doses of each? There are two other ways of specifying which columns to gather. The help for ?gather tells you how to do this: ... Specification of columns to gather. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation. So, we could accomplish the same thing by doing this: hr %&gt;% gather(key=drugdose, value=hr, a_10:c_20) But what if we didn’t know the drug names or doses, but we did know that the only other column in there that we don’t want to gather is name? hr %&gt;% gather(key=drugdose, value=hr, -name) 5.3.2 separate() Finally, look at the help for ?separate. We can pipe in data and omit the first argument. The second argument is the column to separate; the into argument is a character vector of the new column names, and the sep argument is a character used to separate columns, or a number indicating the position to split at. Side note, and 60-second lesson on vectors: We can create arbitrary-length vectors, which are simply variables that contain an arbitrary number of values. To create a numeric vector, try this: c(5, 42, 22908). That creates a three element vector. Try c(&quot;cat&quot;, &quot;dog&quot;). hr %&gt;% gather(key=drugdose, value=hr, -name) %&gt;% separate(drugdose, into=c(&quot;drug&quot;, &quot;dose&quot;), sep=&quot;_&quot;) 5.3.3 %&gt;% it all together Let’s put it all together with gather %&gt;% separate %&gt;% filter %&gt;% group_by %&gt;% summarize. If we create a new data frame that’s a tidy version of hr, we can do those kinds of manipulations we talked about before: # Create a new data.frame hrtidy &lt;- hr %&gt;% gather(key=drugdose, value=hr, -name) %&gt;% separate(drugdose, into=c(&quot;drug&quot;, &quot;dose&quot;), sep=&quot;_&quot;) # Optionally, view it # View(hrtidy) # filter hrtidy %&gt;% filter(drug==&quot;a&quot;) hrtidy %&gt;% filter(dose==20) hrtidy %&gt;% filter(hr&gt;=80) # analyze hrtidy %&gt;% filter(name!=&quot;joe&quot;) %&gt;% group_by(drug, dose) %&gt;% summarize(meanhr=mean(hr)) 5.4 Tidy the yeast data Now, let’s take a look at the yeast data again. The data we’ve been working with up to this point was already cleaned up to a good degree. All of our variables (symbol, nutrient, rate, expression, GO terms, etc.) were each in their own column. Make sure you have the necessary libraries loaded, and read in the tidy data once more into an object called ydat. # Load libraries library(readr) library(dplyr) library(tidyr) # Import data ydat &lt;- read_csv(&quot;data/brauer2007_tidy.csv&quot;) # Optionally, View # View(ydat) # Or just display to the screen ydat But let’s take a look to see what this data originally looked like. yorig &lt;- read_csv(&quot;data/brauer2007_messy.csv&quot;) # View(yorig) yorig There are several issues here. Multiple variables are stored in one column. The NAME column contains lots of information, split up by ::’s. Nutrient and rate variables are stuck in column headers. That is, the column names contain the values of two variables: nutrient (G, N, P, S, L, U) and growth rate (0.05-0.3). Remember, with tidy data, each column is a variable and each row is an observation. Here, we have not one observation per row, but 36 (6 nutrients \\(\\times\\) 6 rates)! There’s no way we could filter this data by a certain nutrient, or try to calculate statistics between rate and expression. Expression values are scattered throughout the table. Related to the problem above, and just like our heart rate example, expression isn’t a single-column variable as in the cleaned tidy data, but it’s scattered around these 36 columns. Other important information is in a separate table. We’re missing all the gene ontology information we had in the tidy data (no information about biological process (bp) or molecular function (mf)). Let’s tackle these issues one at a time, all on a %&gt;% pipeline. 5.4.1 separate() the NAME Let’s separate() the NAME column into multiple different variables. The first row looks like this: SFB2::YNL049C::1082129 That is, it looks like we’ve got the gene symbol, the systematic name, and some other number (that isn’t discussed in the paper). Let’s separate()! yorig %&gt;% separate(NAME, into=c(&quot;symbol&quot;, &quot;systematic_name&quot;, &quot;somenumber&quot;), sep=&quot;::&quot;) Now, let’s select() out the stuff we don’t want. yorig %&gt;% separate(NAME, into=c(&quot;symbol&quot;, &quot;systematic_name&quot;, &quot;somenumber&quot;), sep=&quot;::&quot;) %&gt;% select(-GID, -YORF, -somenumber, -GWEIGHT) 5.4.2 gather() the data Let’s gather the data from wide to long format so we get nutrient/rate (key) and expression (value) in their own columns. yorig %&gt;% separate(NAME, into=c(&quot;symbol&quot;, &quot;systematic_name&quot;, &quot;somenumber&quot;), sep=&quot;::&quot;) %&gt;% select(-GID, -YORF, -somenumber, -GWEIGHT) %&gt;% gather(key=nutrientrate, value=expression, G0.05:U0.3) And while we’re at it, let’s separate() that newly created key column. Take a look at the help for ?separate again. The sep argument could be a delimiter or a number position to split at. Let’s split after the first character. While we’re at it, let’s hold onto this intermediate data frame before we add gene ontology information. Call it ynogo. ynogo &lt;- yorig %&gt;% separate(NAME, into=c(&quot;symbol&quot;, &quot;systematic_name&quot;, &quot;somenumber&quot;), sep=&quot;::&quot;) %&gt;% select(-GID, -YORF, -somenumber, -GWEIGHT) %&gt;% gather(key=nutrientrate, value=expression, G0.05:U0.3) %&gt;% separate(nutrientrate, into=c(&quot;nutrient&quot;, &quot;rate&quot;), sep=1) 5.4.3 inner_join() to GO It’s rare that a data analysis involves only a single table of data. You normally have many tables that contribute to an analysis, and you need flexible tools to combine them. The dplyr package has several tools that let you work with multiple tables at once. Do a Google image search for “SQL Joins”, and look at RStudio’s Data Wrangling Cheat Sheet to learn more. First, let’s import the dataset that links the systematic name to gene ontology information. It’s the brauer2007_sysname2go.csv file available at the data downloads page. Let’s call the imported data frame sn2go. # Import the data sn2go &lt;- read_csv(&quot;data/brauer2007_sysname2go.csv&quot;) # Take a look # View(sn2go) head(sn2go) Now, look up some help for ?inner_join. Inner join will return a table with all rows from the first table where there are matching rows in the second table, and returns all columns from both tables. Let’s give this a try. yjoined &lt;- inner_join(ynogo, sn2go, by=&quot;systematic_name&quot;) # View(yjoined) yjoined # The glimpse function makes it possible to see a little bit of everything in your data. glimpse(yjoined) Looks like that did it! There are many different kinds of two-table verbs/joins in dplyr. In this example, every systematic name in ynogo had a corresponding entry in sn2go, but if this weren’t the case, those un-annotated genes would have been removed entirely by the inner_join. A left_join would have returned all the rows in ynogo, but would have filled in bp and mf with missing values (NA) when there was no corresponding entry. See also: right_join, semi_join, and anti_join. "],
["data-visualization-with-ggplot2.html", "Chapter 6 Data Visualization with ggplot2 6.1 Review 6.2 About ggplot2 6.3 Plotting bivariate data: continuous Y by continuous X 6.4 Plotting bivariate data: continuous Y by categorical X 6.5 Plotting univariate continuous data 6.6 Publication-ready plots &amp; themes 6.7 Homework", " Chapter 6 Data Visualization with ggplot2 This section will cover fundamental concepts for creating effective data visualization and will introduce tools and techniques for visualizing large, high-dimensional data using R. We will review fundamental concepts for visually displaying quantitative information, such as using series of small multiples, avoiding “chart-junk,” and maximizing the data-ink ratio. We will cover the grammar of graphics (geoms, aesthetics, stats, and faceting), and using the ggplot2 package to create plots layer-by-layer. This lesson assumes a basic familiarity with R, data frames, and manipulating data with dplyr and %&gt;%. 6.1 Review 6.1.1 Gapminder data We’re going to work with a different dataset for this section. It’s a cleaned-up excerpt from the Gapminder data. Download the gapminder.csv data by clicking here or using the link above. Let’s read in the data to an object called gm and take a look with View. Remember, we need to load both the dplyr and readr packages for efficiently reading in and displaying this data. # Load packages library(readr) library(dplyr) # You could read the data directly from the web: # gm &lt;- read_csv(&quot;http://bioconnector.org/workshops/data/gapminder.csv&quot;) # Or better to download the data locally and read the file gm &lt;- read_csv(file=&quot;data/gapminder.csv&quot;) # Show the first few lines of the data gm # Optionally bring up data in a viewer window. # View(gm) This particular excerpt has 1704 observations on six variables: country a categorical variable 142 levels continent, a categorical variable with 5 levels year: going from 1952 to 2007 in increments of 5 years pop: population gdpPercap: GDP per capita lifeExp: life expectancy 6.1.2 dplyr review The dplyr package gives you a handful of useful verbs for managing data. On their own they don’t do anything that base R can’t do. Here are some of the single-table verbs we’ll be working with in this lesson (single-table meaning that they only work on a single table – contrast that to two-table verbs used for joining data together). They all take a data.frame or tbl_df as their input for the first argument, and they all return a data.frame or tbl_df as output. filter(): filters rows of the data where some condition is true select(): selects out particular columns of interest mutate(): adds new columns or changes values of existing columns arrange(): arranges a data frame by the value of a column summarize(): summarizes multiple values to a single value, most useful when combined with… group_by(): groups a data frame by one or more variable. Most data operations are useful done on groups defined by variables in the the dataset. The group_by function takes an existing data frame and converts it into a grouped data frame where summarize() operations are performed by group. Additionally, the %&gt;% operator allows you to “chain” operations together. Rather than nesting functions inside out, the %&gt;% operator allows you to write operations left-to-right, top-to-bottom. Let’s say we wanted to get the average life expectancy and GDP (not GDP per capita) for Asian countries for each year. The %&gt;% would allow us to do this: gm %&gt;% mutate(gdp=gdpPercap*pop) %&gt;% filter(continent==&quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize(mean(lifeExp), mean(gdp)) Instead of this: summarize( group_by( filter( mutate(gm, gdp=gdpPercap*pop), continent==&quot;Asia&quot;), year), mean(lifeExp), mean(gdp)) 6.2 About ggplot2 ggplot2 is a widely used R package that extends R’s visualization capabilities. It takes the hassle out of things like creating legends, mapping other variables to scales like color, or faceting plots into small multiples. We’ll learn about what all these things mean shortly. Where does the “gg” in ggplot2 come from? The ggplot2 package provides an R implementation of Leland Wilkinson’s Grammar of Graphics (1999). The Grammar of Graphics allows you to think beyond the garden variety plot types (e.g. scatterplot, barplot) and the consider the components that make up a plot or graphic, such as how data are represented on the plot (as lines, points, etc.), how variables are mapped to coordinates or plotting shape or color, what transformation or statistical summary is required, and so on. Specifically, ggplot2 allows you to build a plot layer-by-layer by specifying: a geom, which specifies how the data are represented on the plot (points, lines, bars, etc.), aesthetics that map variables in the data to axes on the plot or to plotting size, shape, color, etc., a stat, a statistical transformation or summary of the data applied prior to plotting, facets, which we’ve already seen above, that allow the data to be divided into chunks on the basis of other categorical or continuous variables and the same plot drawn for each chunk. First, a note about qplot(). The qplot() function is a quick and dirty way of making ggplot2 plots. You might see it if you look for help with ggplot2, and it’s even covered extensively in the ggplot2 book. And if you’re used to making plots with built-in base graphics, the qplot() function will probably feel more familiar. But the sooner you abandon the qplot() syntax the sooner you’ll start to really understand ggplot2’s approach to building up plots layer by layer. So we’re not going to use it at all in this class. Finally, see this course’s help page for links to getting more help with ggplot2. 6.3 Plotting bivariate data: continuous Y by continuous X The ggplot function has two required arguments: the data used for creating the plot, and an aesthetic mapping to describe how variables in said data are mapped to things we can see on the plot. First let’s load the package: library(ggplot2) Now, let’s lay out the plot. If we want to plot a continuous Y variable by a continuous X variable we’re probably most interested in a scatter plot. Here, we’re telling ggplot that we want to use the gm dataset, and the aesthetic mapping will map gdpPercap onto the x-axis and lifeExp onto the y-axis. Remember that the variable names are case sensitive! ggplot(gm, aes(x = gdpPercap, y = lifeExp)) When we do that we get a blank canvas with no data showing (you might get an error if you’re using an old version of ggplot2). That’s because all we’ve done is laid out a two-dimensional plot specifying what goes on the x and y axes, but we haven’t told it what kind of geometric object to plot. The obvious choice here is a point. Check out docs.ggplot2.org to see what kind of geoms are available. ggplot(gm, aes(x = gdpPercap, y = lifeExp)) + geom_point() Here, we’ve built our plot in layers. First, we create a canvas for plotting layers to come using the ggplot function, specifying which data to use (here, the gm data frame), and an aesthetic mapping of gdpPercap to the x-axis and lifeExp to the y-axis. We next add a layer to the plot, specifying a geom, or a way of visually representing the aesthetic mapping. Now, the typical workflow for building up a ggplot2 plot is to first construct the figure and save that to a variable (for example, p), and as you’re experimenting, you can continue to re-define the p object as you develop “keeper commands”. First, let’s construct the graphic. Notice that we don’t have to specify x= and y= if we specify the arguments in the correct order (x is first, y is second). p &lt;- ggplot(gm, aes(gdpPercap, lifeExp)) The p object now contains the canvas, but nothing else. Try displaying it by just running p. Let’s experiment with adding points and a different scale to the x-axis. # Experiment with adding poings p + geom_point() # Experiment with a different scale p + geom_point() + scale_x_log10() I like the look of using a log scale for the x-axis. Let’s make that stick. p &lt;- p + scale_x_log10() Now, if we re-ran p still nothing would show up because the p object just contains a blank canvas. Now, re-plot again with a layer of points: p + geom_point() Now notice what I’ve saved to p at this point: only the basic plot layout and the log10 mapping on the x-axis. I didn’t save any layers yet because I want to fiddle around with the points for a bit first. Above we implied the aesthetic mappings for the x- and y- axis should be gdpPercap and lifeExp, but we can also add aesthetic mappings to the geoms themselves. For instance, what if we wanted to color the points by the value of another variable in the dataset, say, continent? p + geom_point(aes(color=continent)) Notice the difference here. If I wanted the colors to be some static value, I wouldn’t wrap that in a call to aes(). I would just specify it outright. Same thing with other features of the points. For example, lets make all the points huge (size=8) blue (color=&quot;blue&quot;) semitransparent (alpha=(1/4)) triangles (pch=17): p + geom_point(color=&quot;blue&quot;, pch=17, size=8, alpha=1/4) Now, this time, let’s map the aesthetics of the point character to certain features of the data. For instance, let’s give the points different colors and character shapes according to the continent, and map the size of the point onto the life Expectancy: p + geom_point(aes(col=continent, shape=continent, size=lifeExp)) Now, this isn’t a great plot because there are several aesthetic mappings that are redundant. Life expectancy is mapped to both the y-axis and the size of the points – the size mapping is superfluous. Similarly, continent is mapped to both the color and the point character (the shape is superfluous). Let’s get rid of that, but let’s make the points a little bigger outsize of an aesthetic mapping. p + geom_point(aes(col=continent), size=3) EXERCISE 1 Re-create this same plot from scratch without saving anything to a variable. That is, start from the ggplot call. Start with the ggplot() function. Use the gm data. Map gdpPercap to the x-axis and lifeExp to the y-axis. Add points to the plot Make the points size 3 Map continent onto the aesthetics of the point Use a log10 scale for the x-axis. 6.3.1 Adding layers Let’s add a fitted curve to the points. Recreate the plot in the p object if you need to. p &lt;- ggplot(gm, aes(gdpPercap, lifeExp)) + scale_x_log10() p + geom_point() + geom_smooth() By default geom_smooth() will try to lowess for data with n&lt;1000 or generalized additive models for data with n&gt;1000. We can change that behavior by tweaking the parameters to use a thick red line, use a linear model instead of a GAM, and to turn off the standard error stripes. p + geom_point() + geom_smooth(lwd=2, se=FALSE, method=&quot;lm&quot;, col=&quot;red&quot;) But let’s add back in our aesthetic mapping to the continents. Notice what happens here. We’re mapping continent as an aesthetic mapping to the color of the points only – so geom_smooth() still works only on the entire data. p + geom_point(aes(color = continent)) + geom_smooth() But notice what happens here: we make the call to aes() outside of the geom_point() call, and the continent variable gets mapped as an aesthetic to any further geoms. So here, we get separate smoothing lines for each continent. Let’s do it again but remove the standard error stripes and make the lines a bit thicker. p + aes(color = continent) + geom_point() + geom_smooth() p + aes(color = continent) + geom_point() + geom_smooth(se=F, lwd=2) 6.3.2 Faceting Facets display subsets of the data in different panels. There are a couple ways to do this, but facet_wrap() tries to sensibly wrap a series of facets into a 2-dimensional grid of small multiples. Just give it a formula specifying which variables to facet by. We can continue adding more layers, such as smoothing. If you have a look at the help for ?facet_wrap() you’ll see that we can control how the wrapping is laid out. p + geom_point() + facet_wrap(~continent) p + geom_point() + geom_smooth() + facet_wrap(~continent, ncol=1) 6.3.3 Saving plots There are a few ways to save ggplots. The quickest way, that works in an interactive session, is to use the ggsave() function. You give it a file name and by default it saves the last plot that was printed to the screen. p + geom_point() ggsave(file=&quot;myplot.png&quot;) But if you’re running this through a script, the best way to do it is to pass ggsave() the object containing the plot that is meant to be saved. We can also adjust things like the width, height, and resolution. ggsave() also recognizes the name of the file extension and saves the appropriate kind of file. Let’s save a PDF. pfinal &lt;- p + geom_point() + geom_smooth() + facet_wrap(~continent, ncol=1) ggsave(pfinal, file=&quot;myplot.pdf&quot;, width=5, height=15) EXERCISE 2 Make a scatter plot of lifeExp on the y-axis against year on the x. Make a series of small multiples faceting on continent. Add a fitted curve, smooth or lm, with and without facets. Bonus: using geom_line() and and aesthetic mapping country to group=, make a “spaghetti plot”, showing semitransparent lines connected for each country, faceted by continent. Add a smoothed loess curve with a thick (lwd=3) line with no standard error stripe. Reduce the opacity (alpha=) of the individual black lines. Don’t show Oceania countries (that is, filter() the data where continent!=&quot;Oceania&quot; before you plot it). 6.4 Plotting bivariate data: continuous Y by categorical X With the last example we examined the relationship between a continuous Y variable against a continuous X variable. A scatter plot was the obvious kind of data visualization. But what if we wanted to visualize a continuous Y variable against a categorical X variable? We sort of saw what that looked like in the last exercise. year is a continuous variable, but in this dataset, it’s broken up into 5-year segments, so you could almost think of each year as a categorical variable. But a better example would be life expectancy against continent or country. First, let’s set up the basic plot: p &lt;- ggplot(gm, aes(continent, lifeExp)) Then add points: p + geom_point() That’s not terribly useful. There’s a big overplotting problem. We can try to solve with transparency: p + geom_point(alpha=1/4) But that really only gets us so far. What if we spread things out by adding a little bit of horizontal noise (aka “jitter”) to the data. p + geom_jitter() Note that the little bit of horizontal noise that’s added to the jitter is random. If you run that command over and over again, each time it will look slightly different. The idea is to visualize the density at each vertical position, and spreading out the points horizontally allows you to do that. If there were still lots of over-plotting you might think about adding some transparency by setting the alpha= value for the jitter. p + geom_jitter(alpha=1/2) Probably a more common visualization is to show a box plot: p + geom_boxplot() But why not show the summary and the raw data? p + geom_jitter() + geom_boxplot() Notice how in that example we first added the jitter layer then added the boxplot layer. But the boxplot is now superimposed over the jitter layer. Let’s make the jitter layer go on top. Also, go back to just the boxplots. Notice that the outliers are represented as points. But there’s no distinction between the outlier point from the boxplot geom and all the other points from the jitter geom. Let’s change that. Notice the British spelling. p + geom_boxplot(outlier.colour = &quot;red&quot;) + geom_jitter(alpha=1/2) There’s another geom that’s useful here, called a voilin plot. p + geom_violin() p + geom_violin() + geom_jitter(alpha=1/2) Let’s go back to our boxplot for a moment. p + geom_boxplot() This plot would be a lot more effective if the continents were shown in some sort of order other than alphabetical. To do that, we’ll have to go back to our basic build of the plot again and use the reorder function in our original aesthetic mapping. Here, reorder is taking the first variable, which is some categorical variable, and ordering it by the level of the mean of the second variable, which is a continuous variable. It looks like this p &lt;- ggplot(gm, aes(x=reorder(continent, lifeExp), y=lifeExp)) p + geom_boxplot() EXERCISE 3 Make a jittered strip plot of GDP per capita against continent. Make a box plot of GDP per capita against continent. Using a log10 y-axis scale, overlay semitransparent jittered points on top of box plots, where outlying points are colored. BONUS: Try to reorder the continents on the x-axis by GDP per capita. Why isn’t this working as expected? See ?reorder for clues. 6.5 Plotting univariate continuous data What if we just wanted to visualize distribution of a single continuous variable? A histogram is the usual go-to visualization. Here we only have one aesthetic mapping instead of two. p &lt;- ggplot(gm, aes(lifeExp)) p + geom_histogram() When we do this ggplot lets us know that we’re automatically selecting the width of the bins, and we might want to think about this a little further. p + geom_histogram(bins=30) p + geom_histogram(bins=10) p + geom_histogram(bins=200) p + geom_histogram(bins=60) Alternative we could plot a smoothed density curve instead of a histogram: p + geom_density() Back to histograms. What if we wanted to color this by continent? p + geom_histogram(aes(color=continent)) That’s not what we had in mind. That’s just the outline of the bars. We want to change the fill color of the bars. p + geom_histogram(aes(fill=continent)) Well, that’s not exactly what we want either. If you look at the help for ?geom_histogram you’ll see that by default it stacks overlapping points. This isn’t really an effective visualization. Let’s change the position argument. p + geom_histogram(aes(fill=continent), position=&quot;identity&quot;) But the problem there is that the histograms are blocking each other. What if we tried transparency? p + geom_histogram(aes(fill=continent), position=&quot;identity&quot;, alpha=1/3) That’s somewhat helpful, and might work for two distributions, but it gets cumbersome with 5. Let’s go back and try this with density plots, first changing the color of the line: p + geom_density(aes(color=continent)) Then by changing the color of the fill and setting the transparency to 25%: p + geom_density(aes(fill=continent), alpha=1/4) EXERCISE 4 Plot a histogram of GDP Per Capita. Do the same but use a log10 x-axis. Still on the log10 x-axis scale, try a density plot mapping continent to the fill of each density distribution, and reduce the opacity. Still on the log10 x-axis scale, make a histogram faceted by continent and filled by continent. Facet with a single column (see ?facet_wrap for help). Save this figure to a 6x10 PDF file. 6.6 Publication-ready plots &amp; themes Let’s make a plot we made earlier (life expectancy versus the log of GDP per capita with points colored by continent with lowess smooth curves overlaid without the standard error ribbon): p &lt;- ggplot(gm, aes(gdpPercap, lifeExp)) p &lt;- p + scale_x_log10() p &lt;- p + aes(col=continent) + geom_point() + geom_smooth(lwd=2, se=FALSE) Give the plot a title and axis labels: p &lt;- p + ggtitle(&quot;Life expectancy vs GDP by Continent&quot;) p &lt;- p + xlab(&quot;GDP Per Capita (USD)&quot;) + ylab(&quot;Life Expectancy (years)&quot;) By default, the “gray” theme is the usual background (I’ve changed this course website to use the black and white background for all images). p + theme_gray() We could also get a black and white background: p + theme_bw() Or go a step further and remove the gridlines: p + theme_classic() Finally, there’s another package that gives us lots of different themes. Install it if you don’t have it already. Install all its dependencies along with it. install.packages(&quot;ggthemes&quot;, dependencies = TRUE) library(ggthemes) p &lt;- ggplot(gm, aes(gdpPercap, lifeExp)) p &lt;- p + scale_x_log10() p &lt;- p + aes(col=continent) + geom_point() + geom_smooth(lwd=2, se=FALSE) p + theme_excel() p + theme_excel() + scale_colour_excel() p + theme_gdocs() + scale_colour_gdocs() p + theme_stata() + scale_colour_stata() p + theme_wsj() + scale_colour_wsj() p + theme_economist() p + theme_fivethirtyeight() p + theme_tufte() Don’t forget - see this course’s help page for links to getting more help with ggplot2! 6.7 Homework Looking for more practice? Try this homework assignment. "],
["reproducible-research.html", "Chapter 7 Reproducible Research 7.1 Before class 7.2 Who cares about reproducible research? 7.3 RMarkdown 7.4 Authoring RMarkdown documents 7.5 Distributing Analyses: Rpubs 7.6 Further resources 7.7 Homework", " Chapter 7 Reproducible Research Contemporary life science is plagued by reproducibility issues. This workshop covers some of the barriers to reproducible research and how to start to address some of those problems during the data management and analysis phases of the research life cycle. In this workshop we will cover using R and dynamic document generation with RMarkdown and RStudio to weave together reporting text with executable R code to automatically generate reports in the form of PDF, Word, or HTML documents. This lesson assumes a basic familiarity with R, data frames, manipulating data with dplyr and %&gt;%, and plotting with ggplot2. It also assumes that you’ve successfully installed all packages on the R setup page, including the additional steps needed specifically for this class. 7.1 Before class Prior to class, spend a few minutes to learn a little bit about Markdown. All you really need to know is that Markdown is a lightweight markup language that lets you create styled text (like bold, italics, links, etc.) using a very lightweight plain-text syntax: (like **bold**, _italics_, [links](http://bioconnector.org/markdown), etc.). The resulting text file can be rendered into many downstream formats, like PDF (for printing) or HTML (websites). (30 seconds) Read the summary paragraph on the Wikipedia page. (5-10 minutes) Run through this in-browser markdown tutorial: http://markdowntutorial.com/. (5-10 minutes) Go to http://dillinger.io/, an in-browser Markdown editor, and play around. Write a simple markdown document, and export it to HTML and/or PDF. (5-10 minutes) Take a look at the Markdown/Rmarkdown reference for this course: http://bioconnector.org/markdown. Scroll through the Markdown reference, which you’ve already seen by this point, then click the RMarkdown link at the top and skim through this. (0 seconds) No need to look now, but don’t forget that the course help page has some useful resources on Markdown+RMarkdown. 7.2 Who cares about reproducible research? Science is plagued by reproducibility problems. Especially genomics! Scientists in the United States spend $28 billion each year on basic biomedical research that cannot be repeated successfully.1 A reproducibility study in psychology found that only 39 of 100 studies could be reproduced.2 The Journal Nature on the issue of reproducibility:3 “Nature and the Nature research journals will introduce editorial measures to address the problem by improving the consistency and quality of reporting in life-sciences articles… we will give more space to methods sections. We will examine statistics more closely and encourage authors to be transparent, for example by including their raw data.” Nature also released a checklist, unfortunately with wimpy computational check (see #18). On microarray reproducibility:4 18 Nat. Genet. microarray experiments Less than 50% reproducible Problems: Missing data (38%) Missing software/hardware details (50%) Missing method/processing details (66%) NGS: run-of-the-mill variant calling (align, process, call variants):5 299 articles published in 2011 citing the 1000 Genomes project pilot publication Only 19 were NGS studies with similar design Only 10 used tools recommended by 1000G. Only 4 used full 1000G workflow (realignment &amp; quality score recalibration). Consider this figure: How do we reproduce it? What do we need? The data. Data points themselves. Other metadata. The code. Should be readable. Comments in the code / well-documented so a normal person can figure out how it runs. How were the trend lines drawn? What version of software / packages were used? This kind of information is rarely available in scientific publications, but it’s now extraordinarly easy to put this kind of information on the web. Could I replicate Figure 1 from your last publication? If not, what would you and your co-authors need to provide or do so I could replicate Figure 1 from your last publication? As scientists we should aim for robust and reproducible research “Robust research is about doing small things that stack the deck in your favor to prevent mistakes.” —Vince Buffalo, author of Bioinformatics Data Skills (2015). Reproducible research can be repeated by other researchers with the same results. 7.2.1 Reproducibility is hard! Genomics data is too large and high dimensional to easily inspect or visualize. Workflows involve multiple steps and it’s hard to inspect every step. Unlike in the wet lab, we don’t always know what to expect of our genomics data analysis. It can be hard to distinguish good from bad results. Scientific code is usually only run once to generate results for a publication, and is more likely to contain silent bugs. (code that may produces unknowingly incorrect output rather than stopping with an error message). 7.2.2 What’s in it for you? Yeah, it takes a lot of effort to be robust and reproducible. However, it will make your life (and science) easier! Most likely, you will have to re-run your analysis more than once. In the future, you or a collaborator may have to re-visit part of the project. Your most likely collaborator is your future self, and your past self doesn’t answer emails. You can make modularized parts of the project into re-useable tools for the future. Reproducibility makes you easier to work and collaborate with. 7.2.3 Some recommendations for reproducible research Write code for humans, write data for computers. Code should be broken down into small chunks that may be re-used. Make names/variables consistent, distinctive and meaningful. Adopt a style be consistent.6 Write concise and clear comments. Make incremental changes. Work in small steps with frequent feedback. Use version control. See http://swcarpentry.github.io/git-novice/ for resources on version control. Make assertions and be loud, in code and in your methods. Add tests in your code to make sure it’s doing what you expect. See http://software-carpentry.org/v4/test/ for resources on testing code. Use existing libraries (packages) whenever possible. Don’t reinvent the wheel. Use functions that have already been developed and tested by others. Prevent catastrophe and help reproducibility by making your data read-only. Rather than modifying your original data directly, always use a workflow that reads in data, processes/modifies, then writes out intermediate and final files as necessary. Encapsulate the full project into one directory that is supported with version control. See: Noble, William Stafford. “A quick guide to organizing computational biology projects.” PLoS Comput Biol 5.7 (2009): e1000424. Release your code and data. Simple. Without your code and data, your research is not reproducible. GitHub (https://github.com/) is a great place for storing, distributing, collaborating, and version-controlling code. RPubs (http://rpubs.com/) allows you to share dynamic documents you write in RStudio online. Figshare (http://figshare.com/) and Zenodo (https://zenodo.org/) allow you to upload any kind of research output, publishable or not, free and unlimited. Instantly get permanently available, citable DOI for your research output. “Data/code is available upon request” or “Data/code is available at the lab’s website” are completely unacceptable in the 21st century. Write code that uses relative paths. Don’t use hard-coded absolute paths (i.e. /Users/stephen/Data/seq-data.csv or C:\\Stephen\\Documents\\Data\\Project1\\data.txt). Put the data in the project directory and reference it relative to where the code is, e.g., data/gapminder.csv, etc. Always set your seed. If you’re doing anything that involves random/monte-carlo approaches, always use set.seed(). Document everything and use code as documentation. Document why you do something, not mechanics. Document your methods and workflows. Document the origin of all data in your project directory. Document when and how you downloaded the data. Record data version info. Record software version info with session_info(). Use dynamic documentation to make your life easier. 7.3 RMarkdown RMarkdown is a variant of Markdown that lets you embed R code chunks that execute when you compile the document. What, what? Markdown? Compile? What’s all this about? 7.3.1 Markdown Ever heard of HTML? It’s what drives the internet. HTML is a markup language - that’s what the ML stands for. The terminology evolved from “marking up” paper manuscripts by editors, where the editor would instruct an author or typesetter how to render the resulting text. Markup languages let you annotate text that you want to display with instructions about how to display it. I emphasize text because this is fundamentally different than word processing. When you use MS Word, for example, you’re creating a special proprietary binary file (the .docx) file that shows you how a document looks. By contrast, writing in a markup language like HTML or Markdown, you’re writing plain old text, using a text editor. The toolchain used to render the markup text into what you see on a display or in a PDF has always been and will always bee free and open. You can learn Markdown in about 5 minutes. Visit bioconnector.org/markdown for a quick-start reference and links to other resources. Let’s open up a web-based Markdown editor like http://dillinger.io/ or use a desktop Markdown editor like MarkdownPad (Windows) or MacDown (Mac). 7.3.2 RMarkdown workflow RMarkdown is an enhanced version of Markdown that lets you embed R code into the document. When the document is compiled/rendered, the R code is executed by R, the output is then automatically rendered as Markdown with the rest of the document. The Markdown is then further processed to final output formats like HTML, PDF, DOCX, etc. Visit bioconnector.org/markdown for a quick reference on RMarkdown. 7.4 Authoring RMarkdown documents 7.4.1 From scratch First, open RStudio. Create a new project. Quit RStudio, then launch RStudio using the project file (.Rproj) you just created. Next, download the gapminder data from bioconnector.org/data. Put this file in your R project directory. Maybe put it in a subdirectory called “data.” Importantly, now your code and data will live in the same place. Let’s create a bare-bones RMarkdown document that compiles to HTML. In RStudio, select File, New File, R Markdown…. Don’t worry about the title and author fields. When the new document launches, select everything then delete it. Let’s author an RMarkdown file from scratch. Save it as fromscratch.Rmd. Hit the Knit HTML button in the editor window. You should see the rendered document pop up. So let’s break that down to see exactly what happened there. Recall the RMarkdown Workflow shown above. You start with an RMarkdown document (Rmd). When you hit the Knit HTML button, The knitr R package parses through your source document and executes all the R code chunks defined by the R code chunk blocks. The source code itself and the results are then turned back into regular markdown, inserted into an intermediate markdown file (.md), and finally rendered into HTML by Pandoc. Try this. Instead of using the button, load the knitr package and just knit the document to markdown format. Run this in the console. library(knitr) knit(&quot;fromscratch.Rmd&quot;) Now, open up that regular markdown file and take a look. # Introduction This is my first RMarkdown document! # Let&#39;s embed some R code Let&#39;s load the **Gapminder** data from &lt;http://bioconnector.org&gt;: ```r library(dplyr) library(readr) gm &lt;- read_csv(&quot;data/gapminder.csv&quot;) head(gm) ``` ``` ## country continent year lifeExp pop gdpPercap ## 1 Afghanistan Asia 1952 28.801 8425333 779.4453 ## 2 Afghanistan Asia 1957 30.332 9240934 820.8530 ## 3 Afghanistan Asia 1962 31.997 10267083 853.1007 ## 4 Afghanistan Asia 1967 34.020 11537966 836.1971 ## 5 Afghanistan Asia 1972 36.088 13079460 739.9811 ## 6 Afghanistan Asia 1977 38.438 14880372 786.1134 ``` The mean life expectancy is 59.4744394 years. The years surveyed in this data include: 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007. 7.4.2 From a template with YAML metadata Go ahead and start a new R Markdown document. Fill in some title and author information. This is going to put a YAML header in the file that looks something like this: --- title: &quot;Gapminder Analysis&quot; author: &quot;Stephen Turner&quot; date: &quot;January 1, 2016&quot; output: html_document --- The stuff between the three ---s is metadata. You can read more about what kind of metadata can be included in the RMarkdown documentation. Try clicking the little wrench icon and setting some options, like including a table of contents and figure captions. Notice how the metadata front matter changes. --- title: &quot;Gapminder analysis&quot; author: &quot;Stephen Turner&quot; date: &quot;January 1, 2016&quot; output: html_document: fig_caption: yes toc: yes --- Now, delete everything in that document below the metadata header and paste in what we had written before (above). Save this document under a different name (rmdwithmeta.Rmd for example). You’ll now see that your HTML document takes the metadata and makes a nicely formatted title. Let’s add a plot in there. Open up a new R chunk with this: Using RStudio you can fiddle around with different ways to make the graphic and keep the one you want. Maybe it looks like this: 7.4.3 Chunk options You can modify the behavior of an R chunk with options. Options are passed in after a comma on the fence, as shown below. Some commonly used options include: echo: (TRUE by default) whether to include R source code in the output file. results takes several possible values: markup (the default) takes the result of the R evaluation and turns it into markdown that is rendered as usual. hide will hide results. hold will hold all the output pieces and push them to the end of a chunk. Useful if you’re running commands that result in lots of little pieces of output in the same chunk. asis writes the raw results from R directly into the document. Only really useful for tables. include: (TRUE by default) if this is set to FALSE the R code is still evaluated, but neither the code nor the results are returned in the output document. fig.width, fig.height: used to control the size of graphics in the output. Try modifying your first R chunk to use different values for echo, results, and include. See the full list of options here: http://yihui.name/knitr/options/. There are lots! A special note about caching: The cache= option is automatically set to FALSE. That is, every time you render the Rmd, all the R code is run again from scratch. If you use cache=TRUE, for this chunk, knitr will save the results of the evaluation into a directory that you specify. When you re-render the document, knitr will first check if there are previously cached results under the cache directory before really evaluating the chunk; if cached results exist and this code chunk has not been changed since last run (use MD5 sum to verify), the cached results will be (lazy-) loaded, otherwise new cache will be built; if a cached chunk depends on other chunks (see the dependson option) and any one of these chunks has changed, this chunk must be forcibly updated (old cache will be purged). See the documentation for caching. 7.4.4 Tables Read about printing tables at bioconnector.org/markdown. The knitr package that runs the RMarkdown document in the background also has a function called kable that helps with printing tables nicely. It’s only useful when you set echo=FALSE and results='asis'. Try this. Versus this: 7.4.5 Changing output formats Now try this. If you were successfully able to get a LaTeX distribution installed, you can render this document as a PDF instead of HTML. Try changing the line in the metadata from html_document to pdf_document. Notice how the Knit HTML button in RStudio now changes to Knit PDF. Try it. If you didn’t get a LaTeX engine installed this won’t work. Go back to the setup instructions after class to give this a try. 7.5 Distributing Analyses: Rpubs RPubs.com is a free service from RStudio that allows you to seamlessly publish the results of your R analyses online. Sign up for an account at RPubs.com, then sign in on your browser. Make sure your RMarkdown metadata is set to render to HTML rather than PDF. Render the document. Now notice the little Publish button in the HTML viewer pane. Click this. Sign in when asked, and give your document a name (usually the same name as the title of your Rmd). Here are a few examples of documents I’ve published: http://rpubs.com/turnersd/daily_show_guests: Analysis of every guest who’s ever been on The Daily Show with Jon Stewart. http://rpubs.com/turnersd/twoaxes: How to plot two different tracks of data with one axis on the left and one axis on the right. http://rpubs.com/turnersd/anscombe: Analysis of Anscombe’s Quartet data. Note how RPubs doesn’t share your code! RPubs is a great way to share your analysis but doesn’t let you share the source code. This is a huge barrier to reproducibility. There are plenty of ways to do this. One way is to go to gist.github.com and upload your code as a text file, then link back to the gist in your republished RPubs document. 7.6 Further resources See the (R)markdown section on this course’s help page for links to getting more help with reproducible research, Markdown, and RMarkdown. 7.7 Homework Looking for more practice? Try this homework assignment. Freedman, et al. “The economics of reproducibility in preclinical research.” PLoS Biol 13.6 (2015): e1002165.↩ http://www.nature.com/news/first-results-from-psychology-s-largest-reproducibility-test-1.17433↩ http://www.nature.com/news/reproducibility-1.17552↩ Ioannidis, John PA, et al. “Repeatability of published microarray gene expression analyses.” Nature genetics 41.2 (2009): 149-155.↩ Nekrutenko, Anton, and James Taylor. “Next-generation sequencing data interpretation: enhancing reproducibility and accessibility.” Nature Reviews Genetics 13.9 (2012): 667-672.↩ http://adv-r.had.co.nz/Style.html↩ "],
["essential-statistics.html", "Chapter 8 Essential Statistics 8.1 Our data: NHANES 8.2 Descriptive statistics 8.3 Continuous variables 8.4 Discrete variables 8.5 Power &amp; sample size 8.6 Tidying models", " Chapter 8 Essential Statistics This lesson will provide hands-on instruction and exercises covering basic statistical analysis in R. This will cover descriptive statistics, t-tests, linear models, chi-square, clustering, dimensionality reduction, and resampling strategies. We will also cover methods for “tidying” model results for downstream visualization and summarization. Prerequisites: Familiarity with R is required (including working with data frames, installing/using packages, importing data, and saving results); familiarity with dplyr and ggplot2 packages is highly recommended. You must complete the basic R setup here prior to class. This includes installing R, RStudio, and the required packages. Please contact one of the instructors prior to class if you are having difficulty with any of the setup. Please bring your laptop and charger cable to class. Handouts: Download and print out these handouts and bring them to class: Cheat sheet Exercises handout Slides: click here. 8.1 Our data: NHANES 8.1.1 About NHANES The data we’re going to work with comes from the National Health and Nutrition Examination Survey (NHANES) program at the CDC. You can read a lot more about NHANES on the CDC’s website or Wikipedia. NHANES is a research program designed to assess the health and nutritional status of adults and children in the United States. The survey is one of the only to combine both survey questions and physical examinations. It began in the 1960s and since 1999 examines a nationally representative sample of about 5,000 people each year. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The physical exam includes medical, dental, and physiological measurements, as well as several standard laboratory tests. NHANES is used to determine the prevalence of major diseases and risk factors for those diseases. NHANES data are also the basis for national standards for measurements like height, weight, and blood pressure. Data from this survey is used in epidemiology studies and health sciences research, which help develop public health policy, direct and design health programs and services, and expand the health knowledge for the Nation. We are using a small slice of this data. We’re only using a handful of variables from the 2011-2012 survey years on about 5,000 individuals. The CDC uses a sampling strategy to purposefully oversample certain subpopulations like racial minorities. Naive analysis of the original NHANES data can lead to mistaken conclusions because the percentages of people from each racial group in the data are different from general population. The 5,000 individuals here are resampled from the larger NHANES study population to undo these oversampling effects, so you can treat this as if it were a simple random sample from the American population. You can download the data at the link above. The file is called nhanes.csv. There’s also a data dictionary (filename nhanes_dd.csv) that lists and describes each variable in our NHANES dataset. This table is copied below. 8.1.2 Import &amp; inspect There are some built-in functions for reading in data in text files. These functions are read-dot-something – for example, read.csv() reads in comma-delimited text data; read.delim() reads in tab-delimited text, etc. Normally I would use instead use the read-underscore-something functions like read_csv() or read_tsv() from the readr package. Compared to the base functions, they’re much faster, they’re good at guessing the types of data in the columns, and they have more sensible defaults than the equivalent base functions do. If you have the dplyr package loaded as well, and you read in the data with readr, the data will display nicely. We’re going to be working with factors throughout this lesson. Factors are R’s way of representing categorical variables that can take on a discrete, distinct set of values, e.g., “Yes/No”, “Mutant/WT”, “Case/Control”, etc. The readr package’s read_* functions don’t allow you to convert strings like “Yes/No” to factors automatically. But the base read.* functions do this automatically (not always the desired behavior). We’ll want to use the dplyr package throughout, so let’s go ahead and load it. library(dplyr) If you see a warning that looks like this: Error in library(dplyr) : there is no package called 'dplyr', then you don’t have the package installed correctly. See the setup page. Now, let’s actually load the data. You can get help for the import function with ?read.csv. When we load data we assign it to a variable just like any other, and we can choose a name for that data. Since we’re going to be referring to this data a lot, let’s give it a short easy name to type. I’m going to call it nh. Once we’ve loaded it we can type the name of the object itself (nh) to see it printed to the screen. However, dplyr can convert your “regular” data frame to a “tibble,” which prints more nicely. nh &lt;- read.csv(file=&quot;data/nhanes.csv&quot;) nh &lt;- tbl_df(nh) nh Alternatively, you could still read in the data with readr’s read_csv(), but you’ll want to convert the character variables to factors for the remainder of the lesson. library(readr) nh &lt;- read_csv(&quot;data/nhanes.csv&quot;) %&gt;% mutate_if(is.character, factor) nh Take a look at that output. The nice thing about loading dplyr and wrapping the object with tbl_df() is that data frames are displayed in a much more friendly way. This dataset has 5,000 rows and 32 columns. When you import/convert data this way and try to display the object in the console, instead of trying to display all 5,000 rows, you’ll only see about 10 by default. Also, if you have so many columns that the data would wrap off the edge of your screen, those columns will not be displayed, but you’ll see at the bottom of the output which, if any, columns were hidden from view. If you want to see the whole dataset, there are two ways to do this. First, you can click on the name of the data.frame in the Environment panel in RStudio. Or you could use the View() function (with a capital V). View(nh) Recall several built-in functions that are useful for working with data frames. Content: head(): shows the first few rows tail(): shows the last few rows Size: dim(): returns a 2-element vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object) nrow(): returns the number of rows ncol(): returns the number of columns Summary: colnames() (or just names()): returns the column names glimpse() (from dplyr): Returns a glimpse of your data, telling you the structure of the dataset and information about the class, length and content of each column head(nh) tail(nh) dim(nh) names(nh) glimpse(nh) 8.2 Descriptive statistics We can access individual variables within a data frame using the $ operator, e.g., mydataframe$specificVariable. Let’s print out all the Race values in the data. Let’s then see what are the unique values of each. Then let’s calculate the mean, median, and range of the Age variable. # Display all Race values nh$Race # Get the unique values of Race unique(nh$Race) length(unique(nh$Race)) # Do the same thing the dplyr way nh$Race %&gt;% unique() nh$Race %&gt;% unique() %&gt;% length() # Age mean, median, range mean(nh$Age) median(nh$Age) range(nh$Age) The summary() function (note, this is different from dplyr’s summarize()) works differently depending on which kind of object you pass to it. If you run summary() on a data frame, you get some very basic summary statistics on each variable in the data. summary(nh) 8.2.1 Missing data Let’s try taking the mean of a different variable. mean(nh$Income) What happened there? NA indicates missing data. Take a look at the Income variable. # Look at just the Income variable nh$Income # Or view the dataset # View(nh) Notice that there are lots of missing values for Income. Trying to get the mean a bunch of observations with some missing data returns a missing value by default. This is almost universally the case with all summary statistics – a single NA will cause the summary to return NA. Now look at the help for ?mean. Notice the na.rm argument. This is a logical (i.e., TRUE or FALSE) value indicating whether or not missing values should be removed prior to computing the mean. By default, it’s set to FALSE. Now try it again. mean(nh$Income, na.rm=TRUE) The is.na() function tells you if a value is missing. Get the sum() of that vector, which adds up all the TRUEs to tell you how many of the values are missing. is.na(nh$Income) sum(is.na(nh$Income)) There are a few handy functions in the Tmisc package for summarizing missingness in a data frame. You can graphically display missingness in a data frame as holes on a black canvas with gg_na() (use ggplot2 to plot NA values), or show a table of all the variables and the missingness level with propmiss(). # Install if you don&#39;t have it already # install.packages(&quot;Tmisc&quot;) # Load Tmisc library(Tmisc) gg_na(nh) propmiss(nh) Now, let’s talk about exploratory data analysis (EDA). 8.2.2 EDA It’s always worth examining your data visually before you start any statistical analysis or hypothesis testing. We could spend an entire day on exploratory data analysis. The data visualization lesson covers this in much broader detail. Here we’ll just mention a few of the big ones: histograms and scatterplots. 8.2.2.1 Histograms We can learn a lot from the data just looking at the value distributions of particular variables. Let’s make some histograms with ggplot2. Looking at BMI shows a few extreme outliers. Looking at weight initially shows us that the units are probably in kg. Replotting that in lbs with more bins shows a clear bimodal distribution. Are there kids in this data? The age distribution shows us the answer is yes. library(ggplot2) ggplot(nh, aes(BMI)) + geom_histogram() ggplot(nh, aes(Weight)) + geom_histogram() # In pounds, more bins ggplot(nh, aes(Weight*2.2)) + geom_histogram(bins=80) ggplot(nh, aes(Age)) + geom_histogram() 8.2.2.2 Scatterplots Let’s look at how a few different variables relate to each other. E.g., height and weight: ggplot(nh, aes(Height, Weight, col=Gender)) + geom_point() Let’s filter out all the kids, draw trend lines using a linear model: nh %&gt;% filter(Age&gt;=18) %&gt;% ggplot(aes(Height, Weight, col=Gender)) + geom_point() + geom_smooth(method=&quot;lm&quot;) Check out the data visualization lesson for much more on this topic. 8.2.3 Exercise set 1 What’s the mean 60-second pulse rate for all participants in the data? What’s the range of values for diastolic blood pressure in all participants? (Hint: see help for min(), max(), and range() functions, e.g., enter ?range without the parentheses to get help). What are the median, lower, and upper quartiles for the age of all participants? (Hint: see help for median, or better yet, quantile). What’s the variance and standard deviation for income among all participants? 8.3 Continuous variables 8.3.1 T-tests First let’s create a new dataset from nh called nha that only has adults. To prevent us from making any mistakes downstream, let’s remove the nh object. nha &lt;- filter(nh, Age&gt;=18) rm(nh) # View(nha) Let’s do a few two-sample t-tests to test for differences in means between two groups. The function for a t-test is t.test(). See the help for ?t.test. We’ll be using the forumla method. The usage is t.test(response~group, data=myDataFrame). Are there differences in age for males versus females in this dataset? Does BMI differ between diabetics and non-diabetics? Do single or married/cohabitating people drink more alcohol? Is this relationship significant? t.test(Age~Gender, data=nha) t.test(BMI~Diabetes, data=nha) t.test(AlcoholYear~RelationshipStatus, data=nha) See the heading, Welch Two Sample t-test, and notice that the degrees of freedom might not be what we expected based on our sample size. Now look at the help for ?t.test again, and look at the var.equal argument, which is by default set to FALSE. One of the assumptions of the t-test is homoscedasticity, or homogeneity of variance. This assumes that the variance in the outcome (e.g., BMI) is identical across both levels of the predictor (diabetic vs non-diabetic). Since this is rarely the case, the t-test defaults to using the Welch correction, which is a more reliable version of the t-test when the homoscedasticity assumption is violated. 8.3.2 Wilcoxon test Another assumption of the t-test is that data is normally distributed. Looking at the histogram for AlcoholYear shows that this data clearly isn’t. ggplot(nha, aes(AlcoholYear)) + geom_histogram() The Wilcoxon rank-sum test (a.k.a. Mann-Whitney U test) is a nonparametric test of differences in mean that does not require normally distributed data. When data is perfectly normal, the t-test is uniformly more powerful. But when this assumption is violated, the t-test is unreliable. This test is called in a similar way as the t-test. wilcox.test(AlcoholYear~RelationshipStatus, data=nha) The results are still significant, but much less than the p-value reported for the (incorrect) t-test above. 8.3.3 Linear models Analysis of variance and linear modeling are complex topics that deserve an entire semester dedicated to theory, design, and interpretation. A very good resource is An Introduction to Statistical Learning: with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. The PDF of the book and all the R code used throughout are available free on the author’s website. What follows is a necessary over-simplification with more focus on implementation, and less on theory and design. Where t-tests and their nonparametric substitutes are used for assessing the differences in means between two groups, ANOVA is used to assess the significance of differences in means between multiple groups. In fact, a t-test is just a specific case of ANOVA when you only have two groups. And both t-tests and ANOVA are just specific cases of linear regression, where you’re trying to fit a model describing how a continuous outcome (e.g., BMI) changes with some predictor variable (e.g., diabetic status, race, age, etc.). The distinction is largely semantic – with a linear model you’re asking, “do levels of a categorical variable affect the response?” where with ANOVA or t-tests you’re asking, “does the mean response differ between levels of a categorical variable?” Let’s examine the relationship between BMI and relationship status (RelationshipStatus was derived from MaritalStatus, coded as Committed if MaritalStatus is Married or LivePartner, and Single otherwise). Let’s first do this with a t-test, and for now, let’s assume that the variances between groups are equal. t.test(BMI~RelationshipStatus, data=nha, var.equal=TRUE) It looks like single people have a very slightly higher BMI than those in a committed relationship, but the magnitude of the difference is trivial, and the difference is not significant. Now, let’s do the same test in a linear modeling framework. First, let’s create the fitted model and store it in an object called fit. fit &lt;- lm(BMI~RelationshipStatus, data=nha) You can display the object itself, but that isn’t too interesting. You can get the more familiar ANOVA table by calling the anova() function on the fit object. More generally, the summary() function on a linear model object will tell you much more. fit anova(fit) summary(fit) Go back and re-run the t-test assuming equal variances as we did before. Now notice a few things: t.test(BMI~RelationshipStatus, data=nha, var.equal=TRUE) The p-values from all three tests (t-test, ANOVA, and linear regression) are all identical (p=0.1256). This is because they’re all identical: a t-test is a specific case of ANOVA, which is a specific case of linear regression. There may be some rounding error, but we’ll talk about extracting the exact values from a model object later on. The test statistics are all related. The t statistic from the t-test is 1.532, which is the same as the t-statistic from the linear regression. If you square that, you get 2.347, the F statistic from the ANOVA. The t.test() output shows you the means for the two groups, Committed and Single. Just displaying the fit object itself or running summary(fit) shows you the coefficients for a linear model. Here, the model assumes the “baseline” RelationshipStatus level is Committed, and that the intercept in a regression model (e.g., \\(\\beta_{0}\\) in the model \\(Y = \\beta_{0} + \\beta_{1}X\\)) is the mean of the baseline group. Being Single results in an increase in BMI of 0.3413. This is the \\(\\beta_{1}\\) coefficient in the model. You can easily change the ordering of the levels. See the help for ?factor, and check out the new forcats package, which provides tools for manipulating categorical variables. # P-value computed on a t-statistic with 3552 degrees of freedom # (multiply times 2 because t-test is assuming two-tailed) 2*(1-pt(1.532, df=3552)) # P-value computed on an F-test with 1 and 3552 degrees of freedom 1-pf(2.347, df1=1, df2=3552) 8.3.4 ANOVA Recap: t-tests are for assessing the differences in means between two groups. A t-test is a specific case of ANOVA, which is a specific case of a linear model. Let’s run ANOVA, but this time looking for differences in means between more than two groups. Let’s look at the relationship between smoking status (Never, Former, or Current), and BMI. fit &lt;- lm(BMI~SmokingStatus, data=nha) anova(fit) summary(fit) The F-test on the ANOVA table tells us that there is a significant difference in means between current, former, and never smokers (p=\\(4.54 \\times 10^{-8}\\)). However, the linear model output might not have been what we wanted. Because the default handling of categorical variables is to treat the alphabetical first level as the baseline, “Current” smokers are treated as baseline, and this mean becomes the intercept, and the coefficients on “Former” and “Never” describe how those groups’ means differ from current smokers. What if we wanted “Never” smokers to be the baseline, followed by Former, then Current? Have a look at ?factor to relevel the factor levels. # Re-level the SmokingStatus variable nha$SmokingStatus &lt;- factor(nha$SmokingStatus, levels=c(&quot;Never&quot;, &quot;Former&quot;, &quot;Current&quot;)) # Re-fit the model fit &lt;- lm(BMI~SmokingStatus, data=nha) # Show the ANOVA table anova(fit) # Print the full model statistics summary(fit) Notice that the p-value on the ANOVA/regression didn’t change, but the coefficients did. Never smokers are now treated as baseline. The intercept coefficient (28.856) is now the mean for Never smokers. The SmokingStatusFormer coefficient of .309 shows the apparent increase in BMI that former smokers have when compared to never smokers, but that difference is not significant (p=.24). The SmokingStatusCurrent coefficient of -1.464 shows that current smokers actually have a lower BMI than never smokers, and that this decrease is highly significant. Finally, you can do the typical post-hoc ANOVA procedures on the fit object. For example, the TukeyHSD() function will run Tukey’s test (also known as Tukey’s range test, the Tukey method, Tukey’s honest significance test, Tukey’s HSD test (honest significant difference), or the Tukey-Kramer method). Tukey’s test computes all pairwise mean difference calculation, comparing each group to each other group, identifying any difference between two groups that’s greater than the standard error, while controlling the type I error for all multiple comparisons. First run aov() (not anova()) on the fitted linear model object, then run TukeyHSD() on the resulting analysis of variance fit. TukeyHSD(aov(fit)) This shows that there isn’t much of a difference between former and never smokers, but that both of these differ significantly from current smokers, who have significantly lower BMI. Finally, let’s visualize the differences in means between these groups. The NA category, which is omitted from the ANOVA, contains all the observations who have missing or non-recorded Smoking Status. ggplot(nha, aes(SmokingStatus, BMI)) + geom_boxplot() + theme_classic() 8.3.5 Linear regression Linear models are mathematical representations of the process that (we think) gave rise to our data. The model seeks to explain the relationship between a variable of interest, our Y, outcome, response, or dependent variable, and one or more X, predictor, or independent variables. Previously we talked about t-tests or ANOVA in the context of a simple linear regression model with only a single predictor variable, \\(X\\): \\[Y = \\beta_{0} + \\beta_{1}X\\] But you can have multiple predictors in a linear model that are all additive, accounting for the effects of the others: \\[Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\epsilon\\] \\(Y\\) is the response \\(X_{1}\\) and \\(X_{2}\\) are the predictors \\(\\beta_{0}\\) is the intercept, and \\(\\beta_{1}\\), \\(\\beta_{2}\\) etc are coefficients that describe what 1-unit changes in \\(X_{1}\\) and \\(X_{2}\\) do to the outcome variable \\(Y\\). \\(\\epsilon\\) is random error. Our model will not perfectly predict \\(Y\\). It will be off by some random amount. We assume this amount is a random draw from a Normal distribution with mean 0 and standard deviation \\(\\sigma\\). Building a linear model means we propose a linear model and then estimate the coefficients and the variance of the error term. Above, this means estimating \\(\\beta_{0}, \\beta_{1}, \\beta_{2}\\) and \\(\\sigma\\). This is what we do in R. Let’s look at the relationship between height and weight. fit &lt;- lm(Weight~Height, data=nha) summary(fit) The relationship is highly significant (P&lt;\\(2.2 \\times 10^{-16}\\)). The intercept term is not very useful most of the time. Here it shows us what the value of Weight would be when Height=0, which could never happen. The Height coefficient is meaningful – each one unit increase in height results in a 0.92 increase in the corresponding unit of weight. Let’s visualize that relationship: ggplot(nha, aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method=&quot;lm&quot;) By default, this is only going to show the prediction over the range of the data. This is important! You never want to try to extrapolate response variables outside of the range of your predictor(s). For example, the linear model tells us that weight is -73.7kg when height is zero. We can extend the predicted model / regression line past the lowest value of the data down to height=0. The bands on the confidence interval tell us that the model is apparently confident within the regions defined by the gray boundary. But this is silly – we would never see a height of zero, and predicting past the range of the available training data is never a good idea. ggplot(nha, aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method=&quot;lm&quot;, fullrange=TRUE) + xlim(0, NA) + ggtitle(&quot;Friends don&#39;t let friends extrapolate.&quot;) 8.3.6 Multiple regression Finally, let’s do a multiple linear regression analysis, where we attempt to model the effect of multiple predictor variables at once on some outcome. First, let’s look at the effect of physical activity on testosterone levels. Let’s do this with a t-test and linear regression, showing that you get the same results. t.test(Testosterone~PhysActive, data=nha) summary(lm(Testosterone~PhysActive, data=nha)) In both cases, the p-value is significant (p=0.01516), and the result suggest that increased physical activity is associated with increased testosterone levels. Does increasing your physical activity increase your testosterone levels? Or is it the other way – will increased testosterone encourage more physical activity? Or is it none of the above – is the apparent relationship between physical activity and testosterone levels only apparent because both are correlated with yet a third, unaccounted for variable? Let’s throw Age into the model as well. summary(lm(Testosterone~PhysActive+Age, data=nha)) This shows us that after accounting for age that the testosterone / physical activity link is no longer significant. Every 1-year increase in age results in a highly significant decrease in testosterone, and since increasing age is also likely associated with decreased physical activity, perhaps age is the confounder that makes this relationship apparent. Adding other predictors can also swing things the other way. We know that men have much higher testosterone levels than females. Sex is probably the single best predictor of testosterone levels in our dataset. By not accounting for this effect, our unaccounted-for variation remains very high. By accounting for Gender, we now reduce the residual error in the model, and the physical activity effect once again becomes significant. Also notice that our model fits much better (higher R-squared), and is much more significant overall. summary(lm(Testosterone~PhysActive+Age+Gender, data=nha)) We’ve only looked at the summary() and anova() functions for extracting information from an lm class object. There are several other accessor functions that can be used on a linear model object. Check out the help page for each one of these to learn more. coefficients() predict.lm() fitted.values() residuals() 8.3.7 Exercise set 2 Is the average BMI different in single people versus those in a committed relationship? Perform a t-test. The Work variable is coded “Looking” (n=159), “NotWorking” (n=1317), and “Working” (n=2230). Fit a linear model. Assign this to an object called fit. What does the fit object tell you when you display it directly? Run an anova() to get the ANOVA table. Is the model significant? Run a Tukey test to get the pairwise contrasts. (Hint: TukeyHSD() on aov() on the fit). What do you conclude? Instead of thinking of this as ANOVA, think of it as a linear model. After you’ve thought about it, get some summary() statistics on the fit. Do these results jive with the ANOVA model? Examine the relationship between HDL cholesterol levels (HDLChol) and whether someone has diabetes or not (Diabetes). Is there a difference in means between diabetics and nondiabetics? Perform a t-test without a Welch correction (that is, assuming equal variances – see ?t.test for help). Do the same analysis in a linear modeling framework. Does the relationship hold when adjusting for Weight? What about when adjusting for Weight, Age, Gender, PhysActive (whether someone participates in moderate or vigorous-intensity sports, fitness or recreational activities, coded as yes/no). What is the effect of each of these explanatory variables? 8.4 Discrete variables Until now we’ve only discussed analyzing continuous outcomes / dependent variables. We’ve tested for differences in means between two groups with t-tests, differences among means between n groups with ANOVA, and more general relationships using linear regression. In all of these cases, the dependent variable, i.e., the outcome, or \\(Y\\) variable, was continuous, and usually normally distributed. What if our outcome variable is discrete, e.g., “Yes/No”, “Mutant/WT”, “Case/Control”, etc.? Here we use a different set of procedures for assessing significant associations. 8.4.1 Contingency tables The xtabs() function is useful for creating contingency tables from categorical variables. Let’s create a gender by diabetes status contingency table, and assign it to an object called xt. After making the assignment, type the name of the object to view it. xt &lt;- xtabs(~Gender+Diabetes, data=nha) xt There are two useful functions, addmargins() and prop.table() that add more information or manipulate how the data is displayed. By default, prop.table() will divide the number of observations in each cell by the total. But you may want to specify which margin you want to get proportions over. Let’s do this for the first (row) margin. # Add marginal totals addmargins(xt) # Get the proportional table prop.table(xt) # That wasn&#39;t really what we wanted. # Do this over the first (row) margin only. prop.table(xt, margin=1) Looks like men have slightly higher rates of diabetes than women. But is this significant? The chi-square test is used to assess the independence of these two factors. That is, if the null hypothesis that gender and diabetes are independent is true, the we would expect a proportionally equal number of diabetics across each sex. Males seem to be at slightly higher risk than females, but the difference is just short of statistically significant. chisq.test(xt) An alternative to the chi-square test is Fisher’s exact test. Rather than relying on a critical value from a theoretical chi-square distribution, Fisher’s exact test calculates the exact probability of observing the contingency table as is. It’s especially useful when there are very small n’s in one or more of the contingency table cells. Both the chi-square and Fisher’s exact test give us p-values of approximately 0.06. fisher.test(xt) There’s a useful plot for visualizing contingency table data called a mosaic plot. Call the mosaicplot() function on the contingency table object. mosaicplot(xt, main=NA) Let’s create a different contingency table, this time looking at the relationship between race and whether the person had health insurance. Display the table with marginal totals. xt &lt;- xtabs(~Race+Insured, data=nha) addmargins(xt) Let’s do the same thing as above, this time showing the proportion of people in each race category having health insurance. prop.table(xt, margin=1) Now, let’s run a chi-square test for independence. chisq.test(xt) The result is highly significant. In fact, so significant, that the display rounds off the p-value to something like \\(&lt;2.2 \\times 10^{-16}\\). If you look at the help for ?chisq.test you’ll see that displaying the test only shows you summary information, but other components can be accessed. For example, we can easily get the actual p-value, or the expected counts under the null hypothesis of independence. chisq.test(xt)$p.value chisq.test(xt)$expected We can also make a mosaic plot similar to above: mosaicplot(xt, main=NA) Finally, there’s an association plot that shows deviations from independence of rows and columns in a 2-dimensional contingency table. Each cell is represented by a rectangle that has (signed) height proportional to the deviation of the observed from expected counts and width proportional to the expected counts, so that the area of the box is proportional to the difference in observed and expected frequencies. The rectangles in each row are positioned relative to a baseline indicating independence. If the observed frequency of a cell is greater than the expected one, the box rises above the baseline and is shaded black; otherwise, the box falls below the baseline and is shaded red. See the help for ?assocplot. assocplot(xt) 8.4.2 Logistic regression What if we wanted to model the discrete outcome, e.g., whether someone is insured, against several other variables, similar to how we did with multiple linear regression? We can’t use linear regression because the outcome isn’t continuous – it’s binary, either Yes or No. For this we’ll use logistic regression to model the log odds of binary response. That is, instead of modeling the outcome variable, \\(Y\\), directly against the inputs, we’ll model the log odds of the outcome variable. If \\(p\\) is the probability that the individual is insured, then \\(\\frac{p}{1-p}\\) is the odds that person is insured. Then it follows that the linear model is expressed as: \\[log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\] Where \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the increase in the odds of the outcome for every unit increase in \\(x_1\\), and so on. Logistic regression is a type of generalized linear model (GLM). We fit GLM models in R using the glm() function. It works like the lm() function except we specify which GLM to fit using the family argument. Logistic regression requires family=binomial. The typical use looks like this: mod &lt;- glm(y ~ x, data=yourdata, family=&#39;binomial&#39;) summary(mod) Before we fit a logistic regression model let’s relevel the Race variable so that “White” is the baseline. We saw above that people who identify as “White” have the highest rates of being insured. When we run the logistic regression, we’ll get a separate coefficient (effect) for each level of the factor variable(s) in the model, telling you the increased odds that that level has, as compared to the baseline group. nha$Race &lt;- relevel(factor(nha$Race), ref=&quot;White&quot;) Now, let’s fit a logistic regression model assessing how the odds of being insured change with different levels of race. fit &lt;- glm(Insured~Race, data=nha, family=binomial) summary(fit) The Estimate column shows the log of the odds ratio – how the log odds of having health insurance changes at each level of race compared to White. The P-value for each coefficient is on the far right. This shows that every other race has significantly less rates of health insurance coverage. But, as in our multiple linear regression analysis above, are there other important variables that we’re leaving out that could alter our conclusions? Lets add a few more variables into the model to see if something else can explain the apparent Race-Insured association. Let’s add a few things likely to be involved (Age and Income), and something that’s probably irrelevant (hours slept at night). fit &lt;- glm(Insured~Race+Age+Income+SleepHrsNight, data=nha, family=binomial) summary(fit) A few things become apparent: Age and income are both highly associated with whether someone is insured. Both of these variables are highly significant (\\(P&lt;2.2 \\times 10^{-16}\\)), and the coefficient (the Estimate column) is positive, meaning that for each unit increase in one of these variables, the odds of being insured increases by the corresponding amount. Hours slept per night is not meaningful at all. After accounting for age and income, several of the race-specific differences are no longer statistically significant, but others remain so. The absolute value of the test statistic (column called z value) can roughly be taken as an estimate of the “importance” of that variable to the overall model. So, age and income are the most important influences in this model; self-identifying as Hispanic or Mexican are also very highly important, hours slept per night isn’t important at all, and the other race categories fall somewhere in between. There is much more to go into with logistic regression. This lesson only scratches the surface. Missing from this lesson are things like regression diagnostics, model comparison approaches, penalization, interpretation of model coefficients, fitting interaction effects, and much more. Alan Agresti’s Categorical Data Analysis has long been considered the definitive text on this topic. I also recommend Agresti’s Introduction to Categorical Data Analysis (a.k.a. “Agresti lite”) for a gentler introduction. 8.4.3 Exercise set 3 What’s the relationship between diabetes and participating in rigorous physical activity or sports? Create a contingency table with Diabetes status in rows and physical activity status in columns. Display that table with margins. Show the proportions of diabetics and nondiabetics, separately, who are physically active or not. Is this relationship significant? Create two different visualizations showing the relationship. Model the same association in a logistic regression framework to assess the risk of diabetes using physical activity as a predictor. Fit a model with just physical activity as a predictor, and display a model summary. Add gender to the model, and show a summary. Continue adding weight and age to the model. What happens to the gender association? Continue and add income to the model. What happens to the original association with physical activity? 8.5 Power &amp; sample size This is a necessarily short introduction to the concept of power and sample size calculations. Statistical power, also sometimes called sensitivity, is defined as the probability that your test correctly rejects the null hypothesis when the alternative hypothesis is true. That is, if there really is an effect (difference in means, association between categorical variables, etc.), how likely are you to be able to detect that effect at a given statistical significance level, given certain assumptions. Generally there are a few moving pieces, and if you know all but one of them, you can calculate what that last one is. Power: How likely are you to detect the effect? (Usually like to see 80% or greater). N: What is the sample size you have (or require)? Effect size: How big is the difference in means, odds ratio, etc? If we know we want 80% power to detect a certain magnitude of difference between groups, we can calculate our required sample size. Or, if we know we can only collect 5 samples, we can calculate how likely we are to detect a particular effect. Or, we can work to solve the last one - if we want 80% power and we have 5 samples, what’s the smallest effect we can hope to detect? All of these questions require certain assumptions about the data and the testing procedure. Which kind of test is being performed? What’s the true effect size (often unknown, or estimated from preliminary data), what’s the standard deviation of samples that will be collected (often unknown, or estimated from preliminary data), what’s the level of statistical significance needed (traditionally p&lt;0.05, but must consider multiple testing corrections). 8.5.1 T-test power/N The power.t.test() empirically estimates power or sample size of a t-test for differences in means. If we have 20 samples in each of two groups (e.g., control versus treatment), and the standard deviation for whatever we’re measuring is 2.3, and we’re expecting a true difference in means between the groups of 2, what’s the power to detect this effect? power.t.test(n=20, delta=2, sd=2.3) What’s the sample size we’d need to detect a difference of 0.8 given a standard deviation of 1.5, assuming we want 80% power? power.t.test(power=.80, delta=.8, sd=1.5) 8.5.2 Proportions power/N What about a two-sample proportion test (e.g., chi-square test)? If we have two groups (control and treatment), and we’re measuring some outcome (e.g., infected yes/no), and we know that the proportion of infected controls is 80% but 20% in treated, what’s the power to detect this effect in 5 samples per group? power.prop.test(n=5, p1=0.8, p2=0.2) How many samples would we need for 90% power? power.prop.test(power=0.9, p1=0.8, p2=0.2) Also check out the pwr package which has power calculation functions for other statistical tests. Function Power calculations for pwr.2p.test() Two proportions (equal n) pwr.2p2n.test() Two proportions (unequal n) pwr.anova.test() Balanced one way ANOVA pwr.chisq.test() Chi-square test pwr.f2.test() General linear model pwr.p.test() Proportion (one sample) pwr.r.test() Correlation pwr.t.test() T-tests (one sample, 2 sample, paired) pwr.t2n.test() T-test (two samples with unequal n) 8.5.3 Exercise set 4 You’re doing a gene expression experiment. What’s your power to detect a 2-fold change in a gene with a standard deviation of 0.7, given 3 samples? (Note - fold change is usually given on the \\(log_2\\) scale, so a 2-fold change would be a delta of 1. That is, if the fold change is 2x, then \\(log_2(2)=1\\), and you should use 1 in the calculation, not 2). How many samples would you need to have 80% power to detect this effect? You’re doing a population study looking at the effect of a SNP on disease X. Disease X has a baseline prevalence of 5% in the population, but you suspect the SNP might increase the risk of disease X by 10% (this is typical for SNP effects on common, complex diseases). How many samples do you need to have 80% power to detect this effect, given that you want a statistical significance of \\(p&lt;0.001\\)? 8.6 Tidying models We spent a lot of time in other lessons on tidy data, where each column is a variable and each row is an observation. Tidy data is easy to filter observations based on values in a column (e.g., we could get just adult males with filter(nha, Gender==&quot;male&quot; &amp; Age&gt;=18), and easy to select particular variables/features of interest by their column name. Even when we start with tidy data, we don’t end up with tidy models. The output from tests like t.test or lm are not data.frames, and it’s difficult to get the information out of the model object that we want. The broom package bridges this gap. Depending on the type of model object you’re using, broom provides three methods that do different kinds of tidying: tidy: constructs a data frame that summarizes the model’s statistical findings like coefficients and p-values. augment: add columns to the original data that was modeled, like predictions and residuals. glance: construct a concise one-row summary of the model with information like \\(R^2\\) that are computed once for the entire model. Let’s go back to our linear model example. # Try modeling Testosterone against Physical Activity, Age, and Gender. fit &lt;- lm(Testosterone~PhysActive+Age+Gender, data=nha) # See what that model looks like: summary(fit) What if we wanted to pull out the coefficient for Age, or the P-value for PhysActive? It gets pretty gross. We first have to coef(summary(lmfit)) to get a matrix of coefficients, the terms are still stored in row names, and the column names are inconsistent with other packages (e.g. Pr(&gt;|t|) compared to p.value). Yuck! coef(summary(fit))[&quot;Age&quot;, &quot;Estimate&quot;] coef(summary(fit))[&quot;PhysActiveYes&quot;, &quot;Pr(&gt;|t|)&quot;] Instead, you can use the tidy function, from the broom package, on the fit: library(broom) tidy(fit) This gives you a data.frame with all your model results. The row names have been moved into a column called term, and the column names are simple and consistent (and can be accessed using $). These can be manipulated with dplyr just like any other data frame. tidy(fit) %&gt;% filter(term!=&quot;(Intercept)&quot;) %&gt;% select(term, p.value) Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use augment, which augments the original data with information from the model. New columns begins with a . (to avoid overwriting any of the original columns). # Augment the original data augment(fit) %&gt;% head # Plot residuals vs fitted values for males, # colored by Physical Activity, size scaled by age augment(fit) %&gt;% filter(Gender==&quot;male&quot;) %&gt;% ggplot(aes(.fitted, .resid, col=PhysActive, size=Age)) + geom_point() Finally, several summary statistics are computed for the entire regression, such as \\(R^2\\) and the F-statistic. These can be accessed with glance: glance(fit) The broom functions work on a pipe, so you can %&gt;% your model directly to any of the functions like tidy(). Let’s tidy up our t-test: t.test(AlcoholYear~RelationshipStatus, data=nha) t.test(AlcoholYear~RelationshipStatus, data=nha) %&gt;% tidy() …and our Mann-Whitney U test / Wilcoxon rank-sum test: wilcox.test(AlcoholYear~RelationshipStatus, data=nha) wilcox.test(AlcoholYear~RelationshipStatus, data=nha) %&gt;% tidy() …and our Fisher’s exact test on the cross-tabulated data: xtabs(~Gender+Diabetes, data=nha) %&gt;% fisher.test() xtabs(~Gender+Diabetes, data=nha) %&gt;% fisher.test() %&gt;% tidy() …and finally, a logistic regression model: # fit the model and summarize it the usual way glmfit &lt;- glm(Insured~Race, data=nha, family=binomial) summary(glmfit) # tidy it up! tidy(glmfit) # do whatever you want now tidy(glmfit) %&gt;% filter(term!=&quot;(Intercept)&quot;) %&gt;% mutate(logp=-1*log10(p.value)) %&gt;% ggplot(aes(term, logp)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() Check out some of the other broom vignettes on CRAN, and also check out the biobroom package on bioconductor for turning bioconductor objects and analytical results into tidy data frames. "],
["surival-analysis.html", "Chapter 9 Surival Analysis 9.1 Background 9.2 Survival analysis in R 9.3 TCGA", " Chapter 9 Surival Analysis This lesson will provide hands-on instruction and exercises covering survival analysis using R. The data to be used here will come from The Cancer Genome Atlas (TCGA), where we may also cover programmatic access to TCGA through Bioconductor if time allows. Prerequisites: Familiarity with R is required (including working with data frames, installing/using packages, importing data, and saving results); familiarity with dplyr and ggplot2 packages is highly recommended. You must complete the setup here prior to class. This includes installing R, RStudio, and the required packages under the “Survival Analysis” heading. Please contact one of the instructors prior to class if you are having difficulty with any of the setup. Please bring your laptop and charger cable to class. Handouts: Download and print out these handouts and bring them to class: Cheat sheet Background handout Exercises handout 9.1 Background In the class on essential statistics we covered basic categorical data analysis – comparing proportions (risks, rates, etc) between different groups using a chi-square or fisher exact test, or logistic regression. For example, we looked at how the diabetes rate differed between males and females. In this kind of analysis you implicitly assume that the rates are constant over the period of the study, or as defined by the different groups you defined. But, in longitudinal studies where you track samples or subjects from one time point (e.g., entry into a study, diagnosis, start of a treatment) until you observe some outcome event (e.g., death, onset of disease, relapse), it doesn’t make sense to assume the rates are constant. For example: the risk of death after heart surgery is highest immediately post-op, decreases as the patient recovers, then rises slowly again as the patient ages. Or, recurrence rate of different cancers varies highly over time, and depends on tumor genetics, treatment, and other environmental factors. 9.1.1 Definitions Survival analysis lets you analyze the rates of occurrence of events over time, without assuming the rates are constant. Generally, survival analysis lets you model the time until an event occurs,7 or compare the time-to-event between different groups, or how time-to-event correlates with quantitative variables. The hazard is the instantaneous event (death) rate at a particular time point t. Survival analysis doesn’t assume the hazard is constant over time. The cumulative hazard is the total hazard experienced up to time t. The survival function, is the probability an individual survives (or, the probability that the event of interest does not occur) up to and including time t. It’s the probability that the event (e.g., death) hasn’t occured yet. It looks like this, where \\(T\\) is the time of death, and \\(Pr(T&gt;t)\\) is the probability that the time of death is greater than some time \\(t\\). \\(S\\) is a probability, so \\(0 \\leq S(t) \\leq 1\\), since survival times are always positive (\\(T \\geq 0\\)). \\[ S(t) = Pr(T&gt;t) \\] The Kaplan-Meier curve illustrates the survival function. It’s a step function illustrating the cumulative survival probability over time. The curve is horizontal over periods where no event occurs, then drops vertically corresponding to a change in the survival function at each time an event occurs. Censoring is a type of missing data problem unique to survival analysis. This happens when you track the sample/subject through the end of the study and the event never occurs. This could also happen due to the sample/subject dropping out of the study for reasons other than death, or some other loss to followup. The sample is censored in that you only know that the individual survived up to the loss to followup, but you don’t know anything about survival after that.8 Proportional hazards assumption: The main goal of survival analysis is to compare the survival functions in different groups, e.g., leukemia patients as compared to cancer-free controls. If you followed both groups until everyone died, both survival curves would end at 0%, but one group might have survived on average a lot longer than the other group. Survival analysis does this by comparing the hazard at different times over the observation period. Survival analysis doesn’t assume that the hazard is constant, but does assume that the ratio of hazards between groups is constant over time.9 This workshop does not cover methods to deal with non-proportional hazards, or interactions of covariates with the time to event. Proportional hazards regression a.k.a. Cox regression is the most common approach to assess the effect of different variables on survival. 9.1.2 Cox PH Model Kaplan-Meier curves are good for visualizing differences in survival between two categorical groups,10 but they don’t work well for assessing the effect of quantitative variables like age, gene expression, leukocyte count, etc. Cox PH regression can assess the effect of both categorical and continuous variables, and can model the effect of multiple variables at once.11 Cox PH regression models the natural log of the hazard at time t, denoted \\(h(t)\\), as a function of the baseline hazard (\\(h_0(t)\\)) (the hazard for an individual where all exposure variables are 0) and multiple exposure variables \\(x_1\\), \\(x_1\\), \\(...\\), \\(x_p\\). The form of the Cox PH model is: \\[ log(h(t)) = log(h_0(t)) + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p \\] If you exponentiate both sides of the equation, and limit the right hand side to just a single categorical exposure variable (\\(x_1\\)) with two groups (\\(x_1=1\\) for exposed and \\(x_1=0\\) for unexposed), the equation becomes: \\[ h_1(t) = h_0(t) + e^{\\beta_1 x_1} \\] Rearranging that equation lets you estimate the hazard ratio, comparing the exposed to the unexposed individuals at time t: \\[ HR(t) = \\frac{h_0(t) e^{\\beta_1}}{h_0(t)} = e^{\\beta_1} \\] This model shows that the hazard ratio is \\(e^{\\beta_1}\\), and remains constant over time t (hence the name proportional hazards regression). The \\(\\beta\\) values are the regression coefficients that are estimated from the model, and represent the \\(log(Hazard\\, Ratio)\\) for each unit increase in the corresponding predictor variable. The interpretation of the hazards ratio depends on the measurement scale of the predictor variable, but in simple terms, a positive coefficient indicates worse survival and a negative coefficient indicates better survival for the variable in question. 9.2 Survival analysis in R The core survival analysis functions are in the survival package. The survival package is one of the few “core” packages that comes bundled with your basic R installation, so you probably didn’t need to install.packages() it. But, you’ll need to load it like any other library when you want to use it. We’ll also be using several of the tidyverse packages, so let’s load that too. Finally, we’ll also want to load the survminer package, which provides much nicer Kaplan-Meier plots out-of-the-box than what you get out of base graphics. library(tidyverse) library(survival) library(survminer) The core functions we’ll use out of the survival package include: Surv(): Creates a survival object. survfit(): Fits a survival curve using either a formula, of from a previously fitted Cox model. coxph(): Fits a Cox proportional hazards regression model. Other optional functions you might use include: cox.zph(): Tests the proportional hazards assumption of a Cox regression model. survdiff(): Tests for differences in survival between two groups using a log-rank / Mantel-Haenszel test.12 Surv() creates the response variable, and typical usage takes the time to event,13 and whether or not the event occured (i.e., death vs censored). survfit() creates a survival curve that you could then display or plot. coxph() implements the regression analysis, and models specified the same way as in regular linear models, but using the coxph() function. 9.2.1 Getting started We’re going to be using the built-in lung cancer dataset14 that ships with the survival package. You can get some more information about the dataset by running ?lung. The help tells us there are 10 variables in this data: library(survival) ?lung inst: Institution code time: Survival time in days status: censoring status 1=censored, 2=dead age: Age in years sex: Male=1 Female=2 ph.ecog: ECOG performance score (0=good 5=dead) ph.karno: Karnofsky performance score as rated by physician pat.karno: Karnofsky performance score as rated by patient meal.cal: Calories consumed at meals wt.loss: Weight loss in last six months You can access the data just by running lung, as if you had read in a dataset and called it lung. You can operate on it just like any other data frame. head(lung) class(lung) dim(lung) View(lung) 9.2.2 Survival Curves Check out the help for ?Surv. This is the main function we’ll use to create the survival object. You can play fast and loose with how you specify the arguments to Surv. The help tells you that when there are two unnamed arguments, they will match time and event in that order. This is the common shorthand you’ll often see for right-censored data. The alternative lets you specify interval data, where you give it the start and end times (time and time2). If you keep reading you’ll see how Surv tries to guess how you’re coding the status variable. It will try to guess whether you’re using 0/1 or 1/2 to represent censored vs “dead”, respectively.15 Try creating a survival object called s, then display it. If you go back and head(lung) the data, you can see how these are related. It’s a special type of vector that tells you both how long the subject was tracked for, and whether or not the event occured or the sample was censored (shown by the +). s &lt;- Surv(lung$time, lung$status) class(s) s head(lung) Now, let’s fit a survival curve with the survfit() function. See the help for ?survfit. Here we’ll create a simple survival curve that doesn’t consider any different groupings, so we’ll specify just an intercept (e.g., ~1) in the formula that survfit expects. We can do what we just did by “modeling” the survival object s we just created against an intercept only, but from here out, we’ll just do this in one step by nesting the Surv() call within the survfit() call, and similar to how we specify data for linear models with lm(), we’ll use the data= argument to specify which data we’re using. Similarly, we can assign that to another object called sfit (or whatever we wanted to call it). survfit(s~1) survfit(Surv(time, status)~1, data=lung) sfit &lt;- survfit(Surv(time, status)~1, data=lung) sfit Now, that object itself isn’t very interesting. It’s more interesting to run summary on what it creates. This will show a life table. summary(sfit) These tables show a row for each time point where either the event occured or a sample was censored. It shows the number at risk (number still remaining), and the cumulative survival at that instant. What’s more interesting though is if we model something besides just an intercept. Let’s fit survival curves separately by sex. sfit &lt;- survfit(Surv(time, status)~sex, data=lung) sfit summary(sfit) Now, check out the help for ?summary.survfit. You can give the summary() function an option for what times you want to show in the results. Look at the range of followup times in the lung dataset with range(). You can create a sequence of numbers going from one number to another number by increments of yet another number with the seq() function. # ?summary.survfit range(lung$time) seq(0, 1100, 100) And we can use that sequence vector with a summary call on sfit to get life tables at those intervals separately for both males (1) and females (2). From these tables we can start to see that males tend to have worse survival than females. summary(sfit, times=seq(0, 1000, 100)) ## Call: survfit(formula = Surv(time, status) ~ sex, data = lung) ## ## sex=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 138 0 1.0000 0.0000 1.0000 1.000 ## 100 114 24 0.8261 0.0323 0.7652 0.892 ## 200 78 30 0.6073 0.0417 0.5309 0.695 ## 300 49 20 0.4411 0.0439 0.3629 0.536 ## 400 31 15 0.2977 0.0425 0.2250 0.394 ## 500 20 7 0.2232 0.0402 0.1569 0.318 ## 600 13 7 0.1451 0.0353 0.0900 0.234 ## 700 8 5 0.0893 0.0293 0.0470 0.170 ## 800 6 2 0.0670 0.0259 0.0314 0.143 ## 900 2 2 0.0357 0.0216 0.0109 0.117 ## 1000 2 0 0.0357 0.0216 0.0109 0.117 ## ## sex=2 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 90 0 1.0000 0.0000 1.0000 1.000 ## 100 82 7 0.9221 0.0283 0.8683 0.979 ## 200 66 11 0.7946 0.0432 0.7142 0.884 ## 300 43 9 0.6742 0.0523 0.5791 0.785 ## 400 26 10 0.5089 0.0603 0.4035 0.642 ## 500 21 5 0.4110 0.0626 0.3050 0.554 ## 600 11 3 0.3433 0.0634 0.2390 0.493 ## 700 8 3 0.2496 0.0652 0.1496 0.417 ## 800 2 5 0.0832 0.0499 0.0257 0.270 ## 900 1 0 0.0832 0.0499 0.0257 0.270 9.2.3 Kaplan-Meier Plots Now that we’ve fit a survival curve to the data it’s pretty easy to visualize it with a Kaplan-Meier plot. Create the survival object if you don’t have it yet, and instead of using summary(), use plot() instead. sfit &lt;- survfit(Surv(time, status)~sex, data=lung) plot(sfit) There are lots of ways to modify the plot produced by base R’s plot() function. You can see more options with the help for ?plot.survfit. We’re not going to go into any more detail here, because there’s another package called survminer that provides a function called ggsurvplot() that makes it much easier to produce publication-ready survival plots, and if you’re familiar with ggplot2 syntax it’s pretty easy to modify. So, let’s load the package and try it out. library(survminer) ggsurvplot(sfit) This plot is substantially more informative by default, just because it automatically color codes the different groups, adds axis labels, and creates and automatic legend. But there’s a lot more you can do pretty easily here. Let’s add confidence intervals, show the p-value for the log-rank test, show a risk table below the plot, and change the colors and the group labels. ggsurvplot(sfit, conf.int=TRUE, pval=TRUE, risk.table=TRUE, legend.labs=c(&quot;Male&quot;, &quot;Female&quot;), legend.title=&quot;Sex&quot;, palette=c(&quot;dodgerblue2&quot;, &quot;orchid2&quot;), main=&quot;Kaplan-Meier Curve for Lung Cancer Survival&quot;, risk.table.height=.15) 9.2.4 Exercise set 1 Take a look at the built in colon dataset. If you type ?colon it’ll ask you if you wanted help on the colon dataset from the survival package, or the colon operator. Click “Chemotherapy for Stage B/C colon cancer”, or be specific with ?survival::colon. This dataset has survival and recurrence information on 929 people from a clinical trial on colon cancer chemotherapy. There are two rows per person, indidicated by the event type (etype) variable – etype==1 indicates that row corresponds to recurrence; etype==2 indicates death. First, let’s filter the data to only include the survival data, not the recurrence data. Let’s call this new object colondeath. The filter() function is in the dplyr library, which you can get by running library(dplyr) or library(tidyverse). If you don’t have dplyr or tidyverse, you can use the base subset() function instead. # using tidyverse/dplyr::filter library(tidyverse) colondeath &lt;- filter(colon, etype==2) # Or, using base subset() # colondeath &lt;- subset(colon, etype==2) head(colondeath) Look at the help for ?colon again. How are sex and status coded? How is this different from the lung data? Using survfit(Surv(..., ...,)~..., data=colondeath), create a survival curve separately for males versus females. Call the resulting object sfit. Run a summary() on this object, showing time points 0, 500, 1000, 1500, and 2000. Do males or females appear to fair better over this time period? ## sex=0 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 445 0 1.000 0.0000 1.000 1.000 ## 500 381 64 0.856 0.0166 0.824 0.889 ## 1000 306 75 0.688 0.0220 0.646 0.732 ## 1500 265 40 0.598 0.0232 0.554 0.645 ## 2000 218 22 0.547 0.0236 0.503 0.596 ## ## sex=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 484 0 1.000 0.0000 1.000 1.000 ## 500 418 65 0.866 0.0155 0.836 0.897 ## 1000 335 83 0.694 0.0210 0.654 0.736 ## 1500 287 46 0.598 0.0223 0.556 0.644 ## 2000 238 25 0.545 0.0227 0.503 0.592 Using the survminer package, plot a Kaplan-Meier curve for this analysis with confidence intervals and showing the p-value. See ?ggsurvplot for help. Is there a significant difference between males and females? Create Kaplan-Meier plot stratifying by: The extent of differentiation (well, moderate, poor), showing the p-value. Whether or not there was detectable cancer in &gt;=4 lymph nodes, showing the p-value and confidence bands. 9.2.5 Cox Regression Kaplan-Meier curves are good for visualizing differences in survival between two categorical groups, and the log-rank test you get when you ask for pval=TRUE is useful for asking if there are differences in survival between different groups. But this doesn’t generalize well for assessing the effect of quantitative variables. Just try creating a K-M plot for the nodes variable, which has values that range from 0-33. What a mess! Don’t do this. ggsurvplot(survfit(Surv(time, status)~nodes, data=colondeath)) At some point using a categorical grouping for K-M plots breaks down, and further, you might want to assess how multiple variables work together to influence survival. For example, you might want to simultaneously examine the effect of race and socioeconomic status, so as to adjust for factors like income, access to care, etc., before concluding that ethnicity influences some outcome. Cox PH regression can assess the effect of both categorical and continuous variables, and can model the effect of multiple variables at once. The coxph() function uses the same syntax as lm(), glm(), etc. The response variable you create with Surv() goes on the left hand side of the formula, specified with a ~. Explanatory variables go on the right side. Let’s go back to the lung cancer data and run a Cox regression on sex. fit &lt;- coxph(Surv(time, status)~sex, data=lung) fit ## Call: ## coxph(formula = Surv(time, status) ~ sex, data = lung) ## ## coef exp(coef) se(coef) z p ## sex -0.531 0.588 0.167 -3.18 0.0015 ## ## Likelihood ratio test=10.6 on 1 df, p=0.00111 ## n= 228, number of events= 165 The exp(coef) column contains \\(e^{\\beta_1}\\) (see background section above for more info). This is the hazard ratio – the multiplicative effect of that variable on the hazard rate (for each unit increase in that variable). So, for a categorical variable like sex, going from male (baseline) to female results in approximately 43% reduction in hazard. You could also flip the sign on the coef column, and take exp(0.551), which you can interpret as being male resulting in a 1.73-fold increase in hazard, or that males die ad approximately 1.73x the rate per unit time as females (females die at 0.551x the rate per unit time as males). Just remember: HR=1: No effect HR&gt;1: Increase in hazard HR&lt;1: Reduction in hazard (protective) You’ll also notice there’s a p-value on the sex term, and a p-value on the overall model. That 0.00111 p-value is really close to the p=0.00131 p-value we saw on the Kaplan-Meier plot. That’s because the KM plot is showing the log-rank test p-value. You can get this out of the Cox model with a call to summary(fit). You can directly calculate the log-rank test p-value using survdiff(). summary(fit) survdiff(Surv(time, status)~sex, data=lung) Let’s create another model where we analyze all the variables in the dataset! This shows us how all the variables, when considered together, act to influence survival. Some are very strong predictors (sex, ECOG score). Interestingly, the Karnofsky performance score as rated by the physician was marginally significant, while the same score as rated by the patient was not. fit &lt;- coxph(Surv(time, status)~sex+age+ph.ecog+ph.karno+pat.karno+meal.cal+wt.loss, data=lung) fit ## Call: ## coxph(formula = Surv(time, status) ~ sex + age + ph.ecog + ph.karno + ## pat.karno + meal.cal + wt.loss, data = lung) ## ## coef exp(coef) se(coef) z p ## sex -5.51e-01 5.76e-01 2.01e-01 -2.74 0.0061 ## age 1.06e-02 1.01e+00 1.16e-02 0.92 0.3591 ## ph.ecog 7.34e-01 2.08e+00 2.23e-01 3.29 0.0010 ## ph.karno 2.25e-02 1.02e+00 1.12e-02 2.00 0.0457 ## pat.karno -1.24e-02 9.88e-01 8.05e-03 -1.54 0.1232 ## meal.cal 3.33e-05 1.00e+00 2.60e-04 0.13 0.8979 ## wt.loss -1.43e-02 9.86e-01 7.77e-03 -1.84 0.0652 ## ## Likelihood ratio test=28.3 on 7 df, p=0.000192 ## n= 168, number of events= 121 ## (60 observations deleted due to missingness) 9.2.6 Exercise set 2 Let’s go back to the colon cancer dataset. Remember, you created a colondeath object in the first exercise that only includes survival (etype==2), not recurrence data points. See ?colon for more information about this dataset. Take a look at levels(colondeath$rx). This tells you that the rx variable is the type of treatment the patient was on, which is either nothing (coded Obs, short for Observation), Levamisole (coded Lev), or Levamisole + 5-fluorouracil (coded Lev+5FU). This is a factor variable coded with these levels, in that order. This means that Obs is treated as the baseline group, and other groups are dummy-coded to represent the respective group. Run a Cox proportional hazards regression model against this rx variable. How do you interpret the result? Which treatment seems to be significantly different from the control (Observation)? ## coef exp(coef) se(coef) z p ## rxLev -0.0266 0.9737 0.1103 -0.24 0.8092 ## rxLev+5FU -0.3717 0.6896 0.1188 -3.13 0.0017 ## ## Likelihood ratio test=12.2 on 2 df, p=0.0023 ## n= 929, number of events= 452 Show the results using a Kaplan-Meier plot, with confidence intervals and the p-value. Fit another Cox regression model accounting for age, sex, and the number of nodes with detectable cancer. Notice the test statistic on the likelihood ratio test becomes much larger, and the overall model becomes more significant. What do you think accounted for this increase in our ability to model survival? ## coef exp(coef) se(coef) z p ## rxLev -0.08007 0.92305 0.11161 -0.72 0.47312 ## rxLev+5FU -0.40253 0.66863 0.12054 -3.34 0.00084 ## age 0.00533 1.00535 0.00405 1.32 0.18739 ## sex -0.02826 0.97214 0.09573 -0.30 0.76786 ## nodes 0.09275 1.09719 0.00887 10.46 &lt; 2e-16 ## ## Likelihood ratio test=87.8 on 5 df, p=0 ## n= 911, number of events= 441 ## (18 observations deleted due to missingness) 9.2.7 Categorizing for KM plots Let’s go back to the lung data and look at a Cox model for age. Looks like age is very slightly significant when modeled as a continuous variable. coxph(Surv(time, status)~age, data=lung) Now that your regression analysis shows you that age is marginally significant, let’s make a Kaplan-Meier plot. But, as we saw before, we can’t just do this, because we’ll get a separate curve for every unique value of age! ggsurvplot(survfit(Surv(time, status)~age, data=lung)) One thing you might see here is an attempt to categorize a continuous variable into different groups – tertiles, upper quartile vs lower quartile, a median split, etc – so you can make the KM plot. But, how you make that cut is meaningful! Check out the help for ?cut. cut() takes a continuous variable and some breakpoints and creats a categorical variable from that. Let’s get the average age in the dataset, and plot a histogram showing the distribution of age. mean(lung$age) hist(lung$age) Now, let’s try creating a categorical variable on lung$age with cut pounts at 0, 62 (the mean), and +Infinity (no upper limit). We could continue adding a labels= option here to label the groupings we create, for instance, as “young” and “old”. Finally, we could assign the result of this to a new object in the lung dataset. cut(lung$age, breaks=c(0, 62, Inf)) cut(lung$age, breaks=c(0, 62, Inf), labels=c(&quot;young&quot;, &quot;old&quot;)) lung$agecat &lt;- cut(lung$age, breaks=c(0, 62, Inf), labels=c(&quot;young&quot;, &quot;old&quot;)) head(lung) Now, what happens when we make a KM plot with this new categorization? It looks like there’s some differences in the curves between “old” and “young” patients, with older patients having slightly worse survival odds. But at p=.39, the difference in survival between those younger than 62 and older than 62 are not significant. ggsurvplot(survfit(Surv(time, status)~agecat, data=lung), pval=TRUE) But, what if we chose a different cut point, say, 70 years old, which is roughly the cutoff for the upper quartile of the age distribution (see ?quantile). The result is now marginally significant! lung$agecat &lt;- cut(lung$age, breaks=c(0, 70, Inf), labels=c(&quot;young&quot;, &quot;old&quot;)) ggsurvplot(survfit(Surv(time, status)~agecat, data=lung), pval=TRUE) Remember, the Cox regression analyzes the continuous variable over the whole range of its distribution, where the log-rank test on the Kaplan-Meier plot can change depending on how you categorize your continuous variable. They’re answering a similar question in a different way: the regression model is asking, “what is the effect of age on survival?”, while the log-rank test and the KM plot is asking, “are there differences in survival between those less than 70 and those greater than 70 years old?”. 9.3 TCGA The Cancer Genome Atlas (TCGA) is a collaboration between the National Cancer Institute (NCI) and the National Human Genome Research Institute (NHGRI) that collected lots of clinical and genomic data across 33 cancer types. The entire TCGA dataset is over 2 petabytes worth of gene expression, CNV profiling, SNP genotyping, DNA methylation, miRNA profiling, exome sequencing, and other types of data. You can learn more about TCGA at cancergenome.nih.gov. The data is now housed at the Genomic Data Commons Portal. There are lots of ways to access TCGA data without actually downloading and parsing through the data from GDC. We’ll cover more of these below. But first, let’s look at an R package that provides convenient, direct access to TCGA data. 9.3.1 RTCGA The RTCGA package (bioconductor.org/packages/RTCGA) and all the associated data packages provide convenient access to clinical and genomic data in TCGA. Each of the data packages is a separate package, and must be installed (once) individually. # Load the bioconductor installer. # Try http:// if https:// doesn&#39;t work. source(&quot;https://bioconductor.org/biocLite.R&quot;) # Install the main RTCGA package biocLite(&quot;RTCGA&quot;) # Install the clinical and mRNA gene expression data packages biocLite(&quot;RTCGA.clinical&quot;) biocLite(&quot;RTCGA.mRNA&quot;) Let’s load the RTCGA package, and use the infoTCGA() function to get some information about the kind of data available for each cancer type. library(RTCGA) infoTCGA() 9.3.1.1 Survival Analysis with RTCGA Clinical Data Next, let’s load the RTCGA.clinical package and get a little help about what’s available there. library(RTCGA.clinical) ?clinical This tells us all the clinical datasets available for each cancer type. If we just focus on breast cancer, look at how big the data is! There are 1098 rows by 3703 columns in this data alone. Let’s look at some of the variable names, and View() the data. dim(BRCA.clinical) names(BRCA.clinical) View(BRCA.clinical) We’re going to use the survivalTCGA() function from the RTCGA package to pull out survival information from the clinical data. It does this by looking at vital status (dead or alive) and creating a times variable that’s either the days to death or the days followed up before being censored. Look at the help for ?survivalTCGA for more info. You give it a list of clinical datasets to pull from, and a character vector of variables to extract. Let’s look at breast cancer, ovarian cancer, and glioblastoma multiforme. Let’s just extract the cancer type (admin.disease_code). # Create the clinical data clin &lt;- survivalTCGA(BRCA.clinical, OV.clinical, GBM.clinical, extract.cols=&quot;admin.disease_code&quot;) # Show the first few lines head(clin) ## times bcr_patient_barcode patient.vital_status admin.disease_code ## 1 3767 TCGA-3C-AAAU 0 brca ## 2 3801 TCGA-3C-AALI 0 brca ## 3 1228 TCGA-3C-AALJ 0 brca ## 4 1217 TCGA-3C-AALK 0 brca ## 5 158 TCGA-4H-AAAK 0 brca ## 6 1477 TCGA-5L-AAT0 0 brca # How many samples of each type? table(clin$admin.disease_code) ## ## brca gbm ov ## 1098 595 576 Now let’s run a Cox PH model against the disease code. By default it’s going to treat breast cancer as the baseline, because alphabetically it’s first. But you can reorder this if you want with factor(). coxph(Surv(times, patient.vital_status)~admin.disease_code, data=clin) ## Call: ## coxph(formula = Surv(times, patient.vital_status) ~ admin.disease_code, ## data = clin) ## ## coef exp(coef) se(coef) z p ## admin.disease_codegbm 2.887 17.948 0.113 25.6 &lt;2e-16 ## admin.disease_codeov 1.547 4.697 0.115 13.4 &lt;2e-16 ## ## Likelihood ratio test=904 on 2 df, p=0 ## n= 2269, number of events= 847 This tells us that compared to the baseline brca group, GBM patients have a ~18x increase in hazards, and ovarian cancer patients have ~5x worse survival. Let’s create a survival curve, visualize it with a Kaplan-Meier plot, and show a table for the first 5 years survival rates. sfit &lt;- survfit(Surv(times, patient.vital_status)~admin.disease_code, data=clin) summary(sfit, times=seq(0,365*5,365)) ## Call: survfit(formula = Surv(times, patient.vital_status) ~ admin.disease_code, ## data = clin) ## ## admin.disease_code=brca ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 1096 0 1.000 0.00000 1.000 1.000 ## 365 588 13 0.981 0.00516 0.971 0.992 ## 730 413 11 0.958 0.00851 0.942 0.975 ## 1095 304 20 0.905 0.01413 0.878 0.933 ## 1460 207 9 0.873 0.01719 0.840 0.908 ## 1825 136 14 0.799 0.02474 0.752 0.849 ## ## admin.disease_code=gbm ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 595 2 0.9966 0.00237 0.9920 1.0000 ## 365 224 257 0.5110 0.02229 0.4692 0.5567 ## 730 75 127 0.1998 0.01955 0.1649 0.2420 ## 1095 39 31 0.1135 0.01617 0.0858 0.1500 ## 1460 27 9 0.0854 0.01463 0.0610 0.1195 ## 1825 12 9 0.0534 0.01259 0.0336 0.0847 ## ## admin.disease_code=ov ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 576 0 1.000 0.0000 1.000 1.000 ## 365 411 59 0.888 0.0139 0.861 0.915 ## 730 314 55 0.761 0.0198 0.724 0.801 ## 1095 210 59 0.602 0.0243 0.556 0.651 ## 1460 133 49 0.451 0.0261 0.402 0.505 ## 1825 78 39 0.310 0.0260 0.263 0.365 ggsurvplot(sfit, conf.int=TRUE, pval=TRUE) 9.3.1.2 Gene Expression Data Let’s load the gene expression data. library(RTCGA.mRNA) ?mRNA Take a look at the size of the BRCA.mRNA dataset, show a few rows and columns. dim(BRCA.mRNA) BRCA.mRNA[1:5, 1:5] Extra credit assignment: Take a look at the advanced data manipulation and tidy data workshops, and see if you can figure out how to join the gene expression data to the clinical data for any particular cancer type. Similar to how survivalTCGA() was a nice helper function to pull out survival information from multiple different clinical datasets, expressionsTCGA() can pull out specific gene expression measurements across different cancer types. See the help for ?expressionsTCGA. Let’s pull out data for PAX8, GATA-3, and the estrogen receptor genes from breast, ovarian, and endometrial cancer, and plot the expression of each with a box plot. expr &lt;- expressionsTCGA(BRCA.mRNA, OV.mRNA, UCEC.mRNA, extract.cols = c(&quot;PAX8&quot;, &quot;GATA3&quot;, &quot;ESR1&quot;)) head(expr) ## # A tibble: 6 × 5 ## bcr_patient_barcode dataset PAX8 GATA3 ESR1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TCGA-A1-A0SD-01A-11R-A115-07 BRCA.mRNA -0.542 2.87 3.084 ## 2 TCGA-A1-A0SE-01A-11R-A084-07 BRCA.mRNA -0.595 2.17 2.386 ## 3 TCGA-A1-A0SH-01A-11R-A084-07 BRCA.mRNA 0.500 1.32 0.791 ## 4 TCGA-A1-A0SJ-01A-11R-A084-07 BRCA.mRNA -0.589 1.84 2.495 ## 5 TCGA-A1-A0SK-01A-12R-A084-07 BRCA.mRNA -0.965 -6.03 -4.861 ## 6 TCGA-A1-A0SM-01A-11R-A084-07 BRCA.mRNA 0.573 1.80 2.797 table(expr$dataset) ## ## BRCA.mRNA OV.mRNA UCEC.mRNA ## 590 561 54 ggplot(expr, aes(dataset, PAX8, fill=dataset)) + geom_boxplot() ggplot(expr, aes(dataset, GATA3, fill=dataset)) + geom_boxplot() ggplot(expr, aes(dataset, ESR1, fill=dataset)) + geom_boxplot() ggplot(expr, aes(dataset, ESR1, fill=dataset)) + geom_violin() 9.3.2 Exercise set 3 The “KIPAN” cohort (in KIPAN.clinical) is the pan-kidney cohort, consisting of KICH (chromaphobe renal cell carcinoma), KIRC (renal clear cell carcinoma), and KIPR (papillary cell carcinoma). The KIPAN.clinical has KICH.clinical, KIRC.clinical, and KIPR.clinical all combined. Using survivalTCGA(), create a new object called clinkid using the KIPAN.clinical cohort. For the columns to extract, get both the disease code and the patient’s gender (extract.cols=c(&quot;admin.disease_code&quot;, &quot;patient.gender&quot;)). The first few rows will look like this. ## times bcr_patient_barcode patient.vital_status admin.disease_code ## 1 1158 TCGA-KL-8323 1 kich ## 2 4311 TCGA-KL-8324 0 kich ## 3 725 TCGA-KL-8325 1 kich ## 4 3322 TCGA-KL-8326 0 kich ## 5 3553 TCGA-KL-8327 0 kich ## 6 3127 TCGA-KL-8328 0 kich ## patient.gender ## 1 female ## 2 female ## 3 female ## 4 male ## 5 female ## 6 male The xtabs() command will produce tables of counts for categorical variables. Here’s an example for how to use xtabs() for the built-in colon cancer dataset, which will tell you the number of samples split by sex and by treatment. xtabs(~rx+sex, data=colon) ## sex ## rx 0 1 ## Obs 298 332 ## Lev 266 354 ## Lev+5FU 326 282 Use the same command to examine how many samples you have for each kidney sample type, separately by sex. ## patient.gender ## admin.disease_code female male ## kich 51 61 ## kirc 191 346 ## kirp 76 212 Run a Cox PH regression on the cancer type and gender. What’s the effect of gender? Is it significant? How does survival differ by each type? Which has the worst prognosis? ## coef exp(coef) se(coef) z p ## admin.disease_codekirc 1.5929 4.9179 0.3450 4.62 3.9e-06 ## admin.disease_codekirp 0.9962 2.7080 0.3807 2.62 0.0089 ## patient.gendermale -0.0628 0.9391 0.1484 -0.42 0.6721 ## ## Likelihood ratio test=39.4 on 3 df, p=1.4e-08 ## n= 937, number of events= 203 Create survival curves for each different subtype. Produce a Kaplan-Meier plot. Show survival tables each year for the first 5 years. ## Call: survfit(formula = Surv(times, patient.vital_status) ~ admin.disease_code, ## data = clinkid) ## ## admin.disease_code=kich ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 111 0 1.000 0.0000 1.000 1.000 ## 365 86 2 0.980 0.0144 0.952 1.000 ## 730 72 2 0.954 0.0226 0.911 0.999 ## 1095 54 3 0.910 0.0329 0.848 0.977 ## 1460 44 1 0.893 0.0366 0.824 0.967 ## 1825 38 1 0.871 0.0415 0.794 0.957 ## ## admin.disease_code=kirc ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 536 0 1.000 0.0000 1.000 1.000 ## 365 385 49 0.895 0.0142 0.868 0.924 ## 730 313 32 0.816 0.0186 0.781 0.853 ## 1095 250 26 0.744 0.0217 0.703 0.788 ## 1460 181 20 0.678 0.0243 0.633 0.728 ## 1825 112 16 0.606 0.0277 0.554 0.663 ## ## admin.disease_code=kirp ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0 288 0 1.000 0.0000 1.000 1.000 ## 365 145 10 0.941 0.0182 0.906 0.977 ## 730 100 8 0.877 0.0278 0.824 0.933 ## 1095 67 2 0.853 0.0316 0.793 0.917 ## 1460 54 3 0.810 0.0388 0.737 0.889 ## 1825 36 5 0.727 0.0495 0.636 0.831 9.3.3 Other TCGA Resources RTCGA isn’t the only resource providing easy access to TCGA data. In fact, it isn’t even the only R/Bioconductor package. Take a look at some of the other resources shown below. TCGAbiolinks: another R package that allows direct query and analysis from the NCI GDC. R package: bioconductor.org/packages/TCGAbiolinks Paper: Nucleic Acids Research 2015 DOI: 10.1093/nar/gkv1507. cBioPortal: cbioportal.org Nice graphical user interface Quick/easy summary info on patients, demographics, mutations, copy number alterations, etc. Query individual genes, find coexpressed genes Survival analysis against different subtypes, expression, CNAs, etc. OncoLnc: oncolnc.org Focus on survival analysis and RNA-seq data. Simple query interface across all cancers for any mRNA, miRNA, or lncRNA gene (try SERPINA1) Precomputed Cox PH regression for every gene, for every cancer Kaplan-Meier plots produced on demand TANRIC: focus on noncoding RNA MEXPRESS: focus on methylation and gene expression In the medical world, we typically think of survival analysis literally – tracking time until death. But, it’s more general than that – survival analysis models time until an event occurs (any event). This might be death of a biological organism. But it could also be the time until a hardware failure in a mechanical system, time until recovery, time someone remains unemployed after losing a job, time until a ripe tomato is eaten by a grazing deer, time until someone falls asleep in a workshop, etc. Survival analysis also goes by reliability theory in engineering, duration analysis in economics, and event history analysis in sociology.↩ This describes the most common type of censoring – right censoring. Left censoring less commonly occurs when the “start” is unknown, such as when an initial diagnosis or exposure time is unknown.↩ And, following the definitions above, assumes that the cumulative hazard ratio between two groups remains constant over time.↩ And there’s a chi-square-like statistical test for these differences called the log-rank test that compare the survival functions categorical groups.↩ See the multiple regression section of the essential statistics lesson.↩ Cox regression and the logrank test from survdiff are going to give you similar results most of the time. The log-rank test is asking if survival curves differ significantly between two groups. Cox regression is asking which of many categorical or continuous variables significantly affect survival.↩ Surv() can also take start and stop times, to account for left censoring. See the help for ?Surv.↩ Loprinzi et al. Prospective evaluation of prognostic variables from patient-completed questionnaires. North Central Cancer Treatment Group. Journal of Clinical Oncology. 12(3):601-7, 1994.↩ Where “dead” really refers to the occurance of the event (any event), not necessarily death.↩ "],
["rna-seq-analysis.html", "Chapter 10 RNA-Seq Analysis 10.1 Review 10.2 Background 10.3 Import data 10.4 Poor man’s DGE 10.5 DESeq2 analysis 10.6 Data Visualization 10.7 Record sessionInfo()", " Chapter 10 RNA-Seq Analysis This is an introduction to RNAseq analysis involving reading in count data from an RNAseq experiment, exploring the data using base R functions and then analysis with the DESeq2 package. This lesson assumes a basic familiarity with R, data frames, and manipulating data with dplyr and %&gt;%. See also the Bioconductor heading on the setup page – you’ll need a few additional packages that are available through Bioconductor, not CRAN (the installation process is slightly different). Recommended reading prior to class: Conesa et al. A survey of best practices for RNA-seq data analysis. Genome Biology 17:13 (2016). Abstract and introduction sections of Himes et al. “RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells.” PLoS ONE 9.6 (2014): e99625. Soneson et al. “Differential analyses for RNA-seq: transcript-level estimates improve gene-level inferences.” F1000Research 4 (2015). Slides: click here. 10.1 Review 10.1.1 Prerequsite skills R basics Data frames Manipulating data with dplyr and %&gt;% Tidy data &amp; advanced manipulation Data Visualization with ggplot2 10.1.2 Data needed Count matrix (i.e., countData): airway_rawcounts.csv Sample metadata (i.e., colData): airway_metadata.csv Gene Annotation data: annotables_grch37.csv 10.2 Background 10.2.1 The biology The data for this lesson comes from: Himes et al. “RNA-Seq Transcriptome Profiling Identifies CRISPLD2 as a Glucocorticoid Responsive Gene that Modulates Cytokine Function in Airway Smooth Muscle Cells.” PLoS ONE. 2014 Jun 13;9(6):e99625. PMID: 24926665. Glucocorticoids are potent inhibitors of inflammatory processes, and are widely used to treat asthma because of their anti-inflammatory effects on airway smooth muscle (ASM) cells. But what’s the molecular mechanism? This study used RNA-seq to profile gene expression changes in four different ASM cell lines treated with dexamethasone, a synthetic glucocorticoid molecule. They found a number of differentially expressed genes comparing dexamethasone-treated ASM cells to control cells, but focus much of the discussion on a gene called CRISPLD2. This gene encodes a secreted protein known to be involved in lung development, and SNPs in this gene in previous GWAS studies are associated with inhaled corticosteroid resistance and bronchodilator response in asthma patients. They confirmed the upregulated CRISPLD2 mRNA expression with qPCR and increased protein expression using Western blotting. They did their analysis using Tophat and Cufflinks. We’re taking a different approach and using an R package called DESeq2. Click here to read more on DESeq2 and other approaches. 10.2.2 Data pre-processing Analyzing an RNAseq experiment begins with sequencing reads. There are many ways to begin analyzing this data, and you should check out the three papers below to get a sense of other analysis strategies. In the workflow we’ll use here, sequencing reads were aligned to a reference genome, then the number of reads that mapped to each gene region were counted. This is the starting point - a “count matrix,” where each cell indicates the number of reads mapping to a particular gene (in rows) for each sample (in columns). This workflow was chosen because of its widespread use, and because of recent data showing that it’s relatively robust to potential gene length biases [3]. However, there are many well-established alternative analysis paths, and the goal here is to provide a reference point to acquire fundamental skills that will be applicable to other bioinformatics tools and workflows. Griffith, Malachi, et al. “Informatics for RNA sequencing: a web resource for analysis on the cloud.” PLoS Comput Biol 11.8: e1004393 (2015). Conesa, A. et al. “A survey of best practices for RNA-seq data analysis.” Genome Biology 17:13 (2016). Soneson, C., Love, M. I. &amp; Robinson, M. D. “Differential analyses for RNA-seq: transcript-level estimates improve gene-level inferences.” F1000Res. 4:1521 (2016). This data was downloaded from GEO (GSE:GSE52778). You can read more about how the data was processed here. In summary, the process looks something like this. We have reads; we use an aligner and a reference genome to figure out where in the genome the reads come from (chromosome and position), and we cross-reference that against a reference annotation, which tells the chromosome/position location of exons for known genes. 10.2.3 Data structure We’ll come back to this again later, but the data at our starting point looks like this (note: this is a generic schematic - our genes are not actually geneA and geneB, and our samples aren’t called ctrl_1, ctrl_2, etc.): That is, we have two tables: The “count matrix” (called the countData in DESeq-speak) – where genes are in rows and samples are in columns, and the number in each cell is the number of reads that mapped to exons in that gene for that sample: airway_rawcounts.csv. The sample metadata (called the colData in DESeq-speak) – where samples are in rows and metadata about those samples are in columns: airway_metadata.csv. It’s called the colData because this table supplies metadata/information about the columns of the countData matrix. Notice that the first column of colData must match the column names of countData (except the first, which is the gene ID column).16 10.3 Import data First, let’s load the tidyverse library. (This loads readr, dplyr, ggplot2, and several other needed packages). Then let’s import our data with readr’s read_csv() function (note: not read.csv()). Let’s read in the actual count data and the experimental metadata. library(tidyverse) rawcounts &lt;- read_csv(&quot;data/airway_rawcounts.csv&quot;) metadata &lt;- read_csv(&quot;data/airway_metadata.csv&quot;) Now, take a look at each. rawcounts metadata Notice something here. The sample IDs in the metadata sheet (SRR1039508, SRR1039509, etc.) exactly match the column names of the countdata, except for the first column, which contains the Ensembl gene ID. This is important, and we’ll get more strict about it later on. 10.4 Poor man’s DGE Let’s look for differential gene expression. Note: this analysis is for demonstration only. NEVER do differential expression analysis this way! Let’s start with an exercise. 10.4.1 Exercise 1 If we look at our metadata, we see that the control samples are SRR1039508, SRR1039512, SRR1039516, and SRR1039520. This bit of code will take the rawcounts data, mutate() it to add a column called controlmean, then select() only the gene name and this newly created column, and assigning the result to a new object called meancounts. (Hint: rawcounts %&gt;% mutate(...) %&gt;% select(...)) meancounts &lt;- rawcounts %&gt;% mutate(controlmean = (SRR1039508+SRR1039512+SRR1039516+SRR1039520)/4) %&gt;% select(ensgene, controlmean) meancounts Build off of this code, mutate() it once more (prior to the select()) function, to add another column called treatedmean that takes the mean of the expression values of the treated samples. Then select() only the ensgene, controlmean and treatedmean columns, assigning it to a new object called meancounts. It should look like this. Directly comparing the raw counts is going to be problematic if we just happened to sequence one group at a higher depth than another. Later on we’ll do this analysis properly, normalizing by sequencing depth per sample using a better approach. But for now, summarize() the data to show the sum of the mean counts across all genes for each group. Your answer should look like this: How about another? 10.4.2 Exercise 2 Create a scatter plot showing the mean of the treated samples against the mean of the control samples. Wait a sec. There are 60,000-some rows in this data, but I’m only seeing a few dozen dots at most outside of the big clump around the origin. Try plotting both axes on a log scale (hint: ... + scale_..._log10()) We can find candidate differentially expressed genes by looking for genes with a large change between control and dex-treated samples. We usually look at the \\(log_2\\) of the fold change, because this has better mathematical properties. On the absolute scale, upregulation goes from 1 to infinity, while downregulation is bounded by 0 and 1. On the log scale, upregulation goes from 0 to infinity, and downregulation goes from 0 to negative infinity. So, let’s mutate our meancounts object to add a log2foldchange column. Optionally pipe this to View(). meancounts %&gt;% mutate(log2fc=log2(treatedmean/controlmean)) There are a couple of “weird” results. Namely, the NaN (“not a number”) and -Inf (negative infinity) results. The NaN is returned when you divide by zero and try to take the log. The -Inf is returned when you try to take the log of zero. It turns out that there are a lot of genes with zero expression. Let’s filter our meancounts data, mutate it to add the \\(log_2(Fold Change)\\), and when we’re happy with what we see, let’s reassign the result of that operation back to the meancounts object. (Note: this is destructive. If you’re coding interactively like we’re doing now, before you do this it’s good practice to see what the result of the operation is prior to making the reassignment.) # Try running the code first, prior to reassigning. meancounts &lt;- meancounts %&gt;% filter(controlmean&gt;0 &amp; treatedmean&gt;0) %&gt;% mutate(log2fc=log2(treatedmean/controlmean)) meancounts A common threshold used for calling something differentially expressed is a \\(log_2(FoldChange)\\) of greater than 2 or less than -2. Let’s filter the dataset both ways to see how many genes are up or down-regulated. meancounts %&gt;% filter(log2fc&gt;2) meancounts %&gt;% filter(log2fc&lt;(-2)) In total, we’ve got 892 differentially expressed genes, in either direction. 10.4.3 Exercise 3 Look up help on ?inner_join or Google around for help for using dplyr’s inner_join() to join two tables by a common column/key. You downloaded annotables_grch37.csv from the data downloads page on bioconnector.org. Load this data with read_csv() into an object called anno. Pipe it to View() or click on the object in the Environment pane to view the entire dataset. This table links the unambiguous Ensembl gene ID to things like the gene symbol, full gene name, location, Entrez gene ID, etc. anno &lt;- read_csv(&quot;data/annotables_grch37.csv&quot;) anno Take our newly created meancounts object, and arrange() it descending by the absolute value (abs()) of the log2fc column. The results should look like this: Continue on that pipeline, and inner_join() it to the anno data by the ensgene column. Either assign it to a temporary object or pipe the whole thing to View to take a look. What do you notice? Would you trust these results? Why or why not? 10.5 DESeq2 analysis 10.5.1 DESeq2 package Let’s do this the right way. DESeq2 is an R package for analyzing count-based NGS data like RNA-seq. It is available from Bioconductor. Bioconductor is a project to provide tools for analysing high-throughput genomic data including RNA-seq, ChIP-seq and arrays. You can explore Bioconductor packages here. Bioconductor packages usually have great documentation in the form of vignettes. For a great example, take a look at the DESeq2 vignette for analyzing count data. This 40+ page manual is packed full of examples on using DESeq2, importing data, fitting models, creating visualizations, references, etc. Just like R packages from CRAN, you only need to install Bioconductor packages once (instructions here), then load them every time you start a new R session. library(DESeq2) citation(&quot;DESeq2&quot;) Take a second and read through all the stuff that flies by the screen when you load the DESeq2 package. When you first installed DESeq2 it may have taken a while, because DESeq2 depends on a number of other R packages (S4Vectors, BiocGenerics, parallel, IRanges, etc.) Each of these, in turn, may depend on other packages. These are all loaded into your working environment when you load DESeq2. Also notice the lines that start with The following objects are masked from 'package:.... One example of this is the rename() function from the dplyr package. When the S4Vectors package was loaded, it loaded it’s own function called rename(). Now, if you wanted to use dplyr’s rename function, you’ll have to call it explicitly using this kind of syntax: dplyr::rename(). See this Q&amp;A thread for more. 10.5.2 Importing data DESeq works on a particular type of object called a DESeqDataSet. The DESeqDataSet is a single object that contains input values, intermediate calculations like how things are normalized, and all results of a differential expression analysis. You can construct a DESeqDataSet from a count matrix, a metadata file, and a formula indicating the design of the experiment. See the help for ?DESeqDataSetFromMatrix. If you read through the DESeq2 vignette you’ll read about the structure of the data that you need to construct a DESeqDataSet object. DESeqDataSetFromMatrix requires the count matrix (countData argument) to be a matrix or numeric data frame. either the row names or the first column of the countData must be the identifier you’ll use for each gene. The column names of countData are the sample IDs, and they must match the row names of colData (or the first column when tidy=TRUE). colData is an additional dataframe describing sample metadata. Both colData and countData must be regular data.frame objects – they can’t have the special tbl_df class wrapper created when importing with readr::read_*. Let’s look at our rawcounts and metadata again. rawcounts metadata class(rawcounts) class(metadata) Remember, we read in our count data and our metadata using read_csv() which read them in as those “special” dplyr data frames / tbls. We’ll need to convert them back to regular data frames for them to work well with DESeq2. rawcounts &lt;- as.data.frame(rawcounts) metadata &lt;- as.data.frame(metadata) head(rawcounts) head(metadata) class(rawcounts) class(metadata) Let’s check that the column names of our count data (except the first, which is ensgene) are the same as the IDs from our colData. names(rawcounts)[-1] metadata$id names(rawcounts)[-1]==metadata$id all(names(rawcounts)[-1]==metadata$id) Now we can move on to constructing the actual DESeqDataSet object. The last thing we’ll need to specify is a design – a formula which expresses how the counts for each gene depend on the variables in colData. Take a look at metadata again. The thing we’re interested in is the dex column, which tells us which samples are treated with dexamethasone versus which samples are untreated controls. We’ll specify the design with a tilde, like this: design=~dex. (The tilde is the shifted key to the left of the number 1 key on my keyboard. It looks like a little squiggly line). So let’s contruct the object and call it dds, short for our DESeqDataSet. If you get a warning about “some variables in design formula are characters, converting to factors” don’t worry about it. Take a look at the dds object once you create it. dds &lt;- DESeqDataSetFromMatrix(countData=rawcounts, colData=metadata, design=~dex, tidy=TRUE) dds 10.5.3 DESeq pipeline Next, let’s run the DESeq pipeline on the dataset, and reassign the resulting object back to the same variable. Before we start, dds is a bare-bones DESeqDataSet. The DESeq() function takes a DESeqDataSet and returns a DESeqDataSet, but with lots of other information filled in (normalization, dispersion estimates, differential expression results, etc). Notice how if we try to access these objects before running the analysis, nothing exists. sizeFactors(dds) dispersions(dds) results(dds) ## Error in results(dds): couldn&#39;t find results. you should first run DESeq() Here, we’re running the DESeq pipeline on the dds object, and reassigning the whole thing back to dds, which will now be a DESeqDataSet populated with all those values. Get some help on ?DESeq (notice, no “2” on the end). This function calls a number of other functions within the package to essentially run the entire pipeline (normalizing by library size by estimating the “size factors,” estimating dispersion for the negative binomial model, and fitting models and getting statistics for each gene for the design specified when you imported the data). dds &lt;- DESeq(dds) 10.5.4 Getting results Since we’ve got a fairly simple design (single factor, two groups, treated versus control), we can get results out of the object simply by calling the results() function on the DESeqDataSet that has been run through the pipeline. The help page for ?results and the vignette both have extensive documentation about how to pull out the results for more complicated models (multi-factor experiments, specific contrasts, interaction terms, time courses, etc.). Note two things: We’re passing the tidy=TRUE argument, which tells DESeq2 to output the results table with rownames as a first column called ‘row.’ If we didn’t do this, the gene names would be stuck in the row.names, and we’d have a hard time filtering or otherwise using that column. This returns a regular old data frame. Try displaying it to the screen by just typing res. You’ll see that it doesn’t print as nicly as the data we read in with read_csv. We can add this “special” attribute to the raw data returned which just tells R to print it nicely. res &lt;- results(dds, tidy=TRUE) res &lt;- tbl_df(res) res Either click on the res object in the environment pane or pass it to View() to bring it up in a data viewer. Why do you think so many of the adjusted p-values are missing (NA)? Try looking at the baseMean column, which tells you the average overall expression of this gene, and how that relates to whether or not the p-value was missing. Go to the DESeq2 vignette and read the section about “Independent filtering and multiple testing.” The goal of independent filtering is to filter out those tests from the procedure that have no, or little chance of showing significant evidence, without even looking at the statistical result. Genes with very low counts are not likely to see significant differences typically due to high dispersion. This results in increased detection power at the same experiment-wide type I error [i.e., better FDRs]. 10.5.5 Exercise 4 Using a %&gt;%, arrange the results by the adjusted p-value. Continue piping to inner_join(), joining the results to the anno object. See the help for ?inner_join, specifically the by= argument. You’ll have to do something like ... %&gt;% inner_join(anno, by=c(&quot;row&quot;=&quot;ensgene&quot;)). Once you’re happy with this result, reassign the result back to res. It’ll look like this. How many are significant with an adjusted p-value &lt;0.05? (Pipe to filter()). Finally, let’s write out the significant results. See the help for ?write_csv, which is part of the readr package (note: this is not the same as write.csv with a dot.). We can continue that pipe and write out the significant results to a file like so: res %&gt;% filter(padj&lt;0.05) %&gt;% write_csv(&quot;sigresults.csv&quot;) You can open this file in Excel or any text editor (try it now). 10.6 Data Visualization 10.6.1 Plotting counts DESeq2 offers a function called plotCounts() that takes a DESeqDataSet that has been run through the pipeline, the name of a gene, and the name of the variable in the colData that you’re interested in, and plots those values. See the help for ?plotCounts. Let’s first see what the gene ID is for the CRISPLD2 gene using res %&gt;% filter(symbol==&quot;CRISPLD2&quot;). Now, let’s plot the counts, where our intgroup, or “interesting group” variable is the “dex” column. plotCounts(dds, gene=&quot;ENSG00000103196&quot;, intgroup=&quot;dex&quot;) That’s just okay. Keep looking at the help for ?plotCounts. Notice that we could have actually returned the data instead of plotting. We could then pipe this to ggplot and make our own figure. Let’s make a boxplot. # Return the data plotCounts(dds, gene=&quot;ENSG00000103196&quot;, intgroup=&quot;dex&quot;, returnData=TRUE) # Plot it plotCounts(dds, gene=&quot;ENSG00000103196&quot;, intgroup=&quot;dex&quot;, returnData=TRUE) %&gt;% ggplot(aes(dex, count)) + geom_boxplot(aes(fill=dex)) + scale_y_log10() + ggtitle(&quot;CRISPLD2&quot;) 10.6.2 MA &amp; Volcano plots Let’s make some commonly produced visualizations from this data. First, let’s mutate our results object to add a column called sig that evaluates to TRUE if padj&lt;0.05, and FALSE if not, and NA if padj is also NA. # Create the new column res &lt;- res %&gt;% mutate(sig=padj&lt;0.05) # How many of each? res %&gt;% group_by(sig) %&gt;% summarize(n=n()) 10.6.3 Exercise 5 Look up the Wikipedia articles on MA plots and volcano plots. An MA plot shows the average expression on the X-axis and the log fold change on the y-axis. A volcano plot shows the log fold change on the X-axis, and the \\(-log_{10}\\) of the p-value on the Y-axis (the more significant the p-value, the larger the \\(-log_{10}\\) of that value will be). Make an MA plot. Use a \\(log_{10}\\)-scaled x-axis, color-code by whether the gene is significant, and give your plot a title. It should look like this. What’s the deal with the gray points? Make a volcano plot. Similarly, color-code by whether it’s significant or not. 10.6.4 Transformation To test for differential expression we operate on raw counts. But for other downstream analyses like heatmaps, PCA, or clustering, we need to work with transformed versions of the data, because it’s not clear how to best compute a distance metric on untransformed counts. The go-to choice might be a log transformation. But because many samples have a zero count (and \\(log(0)=-\\infty\\), you might try using pseudocounts, i. e. \\(y = log(n + 1)\\) or more generally, \\(y = log(n + n_0)\\), where \\(n\\) represents the count values and \\(n_0\\) is some positive constant. But there are other approaches that offer better theoretical justification and a rational way of choosing the parameter equivalent to \\(n_0\\), and they produce transformed data on the log scale that’s normalized to library size. One is called a variance stabilizing transformation (VST), and it also removes the dependence of the variance on the mean, particularly the high variance of the log counts when the mean is low. vsdata &lt;- vst(dds, blind=FALSE) 10.6.5 PCA Let’s do some exploratory plotting of the data using principal components analysis on the variance stabilized data from above. Let’s use the DESeq2-provided plotPCA function. See the help for ?plotPCA and notice that it also has a returnData option, just like plotCounts. plotPCA(vsdata, intgroup=&quot;dex&quot;) Principal Components Analysis (PCA) is a dimension reduction and visualization technique that is here used to project the multivariate data vector of each sample into a two-dimensional plot, such that the spatial arrangement of the points in the plot reflects the overall data (dis)similarity between the samples. In essence, principal component analysis distills all the global variation between samples down to a few variables called principal components. The majority of variation between the samples can be summarized by the first principal component, which is shown on the x-axis. The second principal component summarizes the residual variation that isn’t explained by PC1. PC2 is shown on the y-axis. The percentage of the global variation explained by each principal component is given in the axis labels. In a two-condition scenario (e.g., mutant vs WT, or treated vs control), you might expect PC1 to separate the two experimental conditions, so for example, having all the controls on the left and all experimental samples on the right (or vice versa - the units and directionality isn’t important). The secondary axis may separate other aspects of the design - cell line, time point, etc. Very often the experimental design is reflected in the PCA plot, and in this case, it is. But this kind of diagnostic can be useful for finding outliers, investigating batch effects, finding sample swaps, and other technical problems with the data. This YouTube video from the Genetics Department at UNC gives a very accessible explanation of what PCA is all about in the context of a gene expression experiment, without the need for an advanced math background. Take a look. 10.6.6 Bonus: Heatmaps Heatmaps are complicated, and are often poorly understood. It’s a type of visualization used very often in high-throughput biology where data are clustered on rows and columns, and the actual data is displayed as tiles on a grid, where the values are mapped to some color spectrum. Our R useRs group MeetUp had a session on making heatmaps, which I summarized in this blog post. Take a look at the code from that meetup, and the documentation for the aheatmap function in the NMF package to see if you can re-create this image. Here, I’m clustering all samples using the top 25 most differentially regulated genes, labeling the rows with the gene symbol, and putting two annotation color bars across the top of the main heatmap panel showing treatment and cell line annotations from our metadata. Take a look at the Rmarkdown source for this lesson for the code. 10.7 Record sessionInfo() The sessionInfo() prints version information about R and any attached packages. It’s a good practice to always run this command at the end of your R session and record it for the sake of reproducibility in the future. sessionInfo() This only works when using the argument tidy=TRUE when creating the DESeqDataSetFromMatrix().↩ "],
["references.html", "References", " References "]
]
